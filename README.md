# MLxE: Highly Parallelizable and Distributed Framework for Implementation of Large-Scale, Multi-Worker Reinforcement Learning Agents and Algorithms

# Objectives of the Project:
- To introduce a Reinforcement Learning (RL) Framework for highly efficient experimentation with and implementation of Reinforcement Learning agents and algorithms.
- To enable RL researchers and practitioners a rapid experimentation, prototyping and architecting of RL Solutions not implemented currently in the industry-ready frameworks such as RLLib
- To enable direct control of all compute resources both on individual machines as well as in the Cloud

# The Key Achievement

Introduction of a novel RL Architecture that allowed rapid Development and Implementation of the A2C Algorithm that features 20-fold improvement in both Training Time and Sample Efficiency when compared with the Base A2C implementation. The MLxE trained agent learns to keep the GYM's CartPole's bar up for 50,000 steps after 6 minutes of learning and experiencing only 140 example trajectories. The Base A2C implementation required 2 hours of training and 3000 examples to achieve comparable "skills". MLxE based A2C algorithm featured also much more stable training, experiencing much less and much lower performance deteriorations during training and maintaining its high-performance when training was allowed to last longer than 6 minutes.

# The MLxE Architecture
- Most modern RL algorithms and architectures take great advantages from or are designed specifically for multi-worker, parallelized and distributed implementations. MLxE Framework allows implementation of specialized workers that focus on specific step of the RL learning process. This allows them to be higly-effcient in execution of that step and to take advantage of the parallelization capabilities of the modern hardware to speed up the training.   

- At its core, MLxE Architecture consist of multiple processes (called Memorizer, Learner and Executors) executed (optimally) in parallel:

M Process - the Memorizer(s): Manages the Experience Buffer (Memory) of the Learning Agent. It collects examples generated by Executors, processes them and batches them for consumption by the Learner

L Process - the Learner(s): The core process of the Learning Agent responsible for updating model weights through (usually) an SGD based algorithm. It consumes the training examples generated by Memorizer and distributes updated models to Executors for generation of new examples.

xE Processes - the Executors: Generate new examples for training utilizing the model computed by the Learner. Usually, there are multiple (x) Executors running in parallel thus "xE" in the name of the architecture.

- Implemented with TensorFlow 2 but can be easily modified to work with all major ML/DL Frameworks (PyTorch,...)
- Cloud Ready: Parallelizable across multiple CPUs, GPUs and Distributable across multiple instances
- Can be implemented with Vectorized Environments (Such as Vectorized-GYM) to enable faster experience generation.
- Each worker type can be modified and enhanced independently from the other worker types and itself can have multi-core implementation.

- Core Parallelization implemented with:
    - Python's Multiprocessing
        See: A3C MLxE-IS - PRB + MMUF + Reversed e-Greedy.py
    - Ray Library & Vectorized Environment 
        See: MLxE Ray Multi_ENV_Per_Actor.ipynb

We propose four sub-architectures tested, compare their relative performance and make recommendation for the best performing one for the A3C-Based Algorithm: The Iterative-Synchronous MLxE:

1.) Iterative-Synchronous Multi-Thread Based (ISTB) - it is our benchmark implementation based on modification of a code available in the book.
- Iterative - learning phase is performed intermittently with the example generation phase
- Synchronous - each executor generates only one example per iteration, waiting idle for closing of all executors and update of the model in given iteration
- Model update for the Learner and Executors is performed at the end of each iteration

2.) Iterative-Synchronous MLxE Based (IS MLxE) - the best performing architecture, used in final implementations of the RL Algorithms
- Implemented with Multiprocessing
- Other details are the same as that of the ISTB architecture
- Due to iterative nature of the architecture, all cores are used as Executors in Example Generation Phase and one process is used subsequently for combined Memorization and Learning Phases 

3.) Iterative-Asynchronous MLxE Based (IA MLxE) - During Example Generation Phase all Executors generate examples till each Executor generates at least one Example. In other words, Executors start generation of another example if they finish their previous example before all Executors finished generating their first example in given iteration.

4.) On-Line-Asynchronous MLxE Based (OA MLxE) - the Memorizing and Learning Phases are spawned in their own processes and are run in parallel to the Executors. The model is continuously updated, fed with freshly generated examples and its new version becomes immediately available to Executors for generation of new examples.

For visual representation of the MLxE Architecture, we provide comparison of the variants of A2C/A3C MLxE to that of the DQN, A3C and the Reactor:

![github-small](https://github.com/sebtac/MLxE/blob/main/MLxE%20vs.%20DQN%2C%20A3C%2C%20REACTOR.jpg)


# Developing and Testing the MLxE Architecture

The detailed discussion of all the implemented and tested elements of this work you can find in the "README - Complete Analysis.md" file. Here, we provide a high-level overview of the work and its key findings.
 
The power of MLxE architecture is showcased with Implementation of A2C/A3C algorithm: 
- The base, non-parallelized implementation of the algorithm is from the Deep Reinforcement Learning book by Mohit Sewak. Its GitHub repository is: https://github.com/mohitsewak/DeepReinforcementLearning
- The tests are performed on RL's "Hello World" task, the GYM's CartPole Environment
- With MLxE, the algorithm was split into Memorizer, Learner and multiple Executor workers.
- A number of potential algorithmic improvements (coming mainly from the RAINBOW paper) was implemented and tested.
- We have also proposed, developed, implemented and tested a number of our original algorithmic enhancements to the Base A2C algorithm.    

In the end, we have architecture an implementation of the A2C Algorithm that features 20-fold improvement in both Training Time and Sample Efficiency when compared with the Base A2C implementation. Our MLxE trained agent learns to keep the CartPole's bar up for 50,000 steps after 6 minutes* of learning and experiencing only 140 example trajectories generated by the Executors. The Base A2C implementation required 2 hours of training and 3000 examples to achieve comparable "skills". MLxE based A2C algorithm featured also much more stable training, experiencing much less and much lower performance deteriorations during training. It also maintained its high-performance when training was allowed to last longer than 6 minutes.

* Tests performed on machine with 8 CPUs and 0 GPUs. To test performance gains from utilizing GPUs and/or machines with larger number of CPUs, we will perform tests with a more involved testing task.

The Best Architecture:
- Iterative-Synchronous MLxE Based (IS MLxE)
- On 8 CPU machine, there were 8 Executors, one of which performed role of the Learner and one of the Memorizer after experience collection step.
- "Task" specific modifications:
    - Modification of the State Representation - Adding squared value of the cart position (env.state[0]) to allow the agent to learn quicker to keep far away from the edges of the plain.
    - Adverse Modification of the Initial Position Parameters - randomly selected adverse initial conditions (cart-pole close to the edge or heavily leaning pole) - this allows the agent to experience the "danger" zones even when it becomes fairly good at controlling the pole. 
    - Modified Reward - Making Reward more "Informative" (Note that only the failure condition results in a true learning feedback (no reward))
        - Failure "Reward" set to -10 (instead of 0) - provides the agent with stronger signal of failure condition
        - Non-constant reward discount factor (the gamma coefficient) - with an algorithm that modifies the value of the discount factor depending on how close given state is to the failure condition. It allows the agent to pay more attention to immediate actions when in "danger" zones and have longer attention span when in safer conditions.
        - Additional rewards representing a relative safety of the state - Bonus for being at the center of the plane and the pole being in vertical position. In effect, the agent can get up to 3 points in each state 
        - The combined effect of the three above Reward Function modifications is that the reward assigned to the states increases monotonically in logarithmic fashion as the state is farther away from the failure condition allowing the agent to get more precise sense of the value of the state it is in.

- Deep Learning Specific Adjustments to the Model and Hyper-Parameters:
    - Increased the batch size to 64
    - Increased the number of Model Update Steps to 64
    - Added monotonic decrease in Learning Rate relative to the number of episodes run with:
        self.alpha_power = 0.998
        self.alpha_limit = 0.000001
    - Introduced Broken-Dimond-Shaped Model Design and increased the network size: common_network_size=[128,258,512], policy_network_size=[256,128,64], value_network_size=[256,128,64]
    - Changed the Optimizer to RectifiedAdam -- requires tensorflow_addons
    - Changed Gamma coefficient to 0.97
    
- Multi-Factor Priority Memory Buffer Weights based on three factors:
    - The “Age” of an example
    - The “Risk Level” of an example
    - The “Current” TD Error of an example

- Memory buffer implemented as NumPy array to take advantage of NumPy’s broadcasting capabilities for rapid updates to the priority weights

- The Reversed E-Greedy Policy (REG Policy)
    - The novel “Reversed” algorithm allows the Greedy Policy to be used more frequently toward the beginning of the training followed by increased use of the "Informed" Random Policy. The “Informed” stands for the use of stochastic policy derived from the current model. We theorize that as the model learns, it becomes better in distinguishing the bad move from the good one thus making such choice more deterministic/greedy. It should also allow for either action to be taken if their relative value is similar in given state leading to wider experience base. Allowing the more frequent use of the Greedy action toward the begging of training makes the learning process more On-Policy like when the accumulated learnings are small and thus stabilizes the training.
 
Other Implemented and Tested Elements (That did not enter to the recommended architecture):
- TD(n)
    - TD(n) had only minor impact with the TD(1) setting. We theorize that the impact of that modification is limited due to Modified Discounted Reward already taking into account the "safety" level of the given state. To avoid additional computational complexities, we have not included this modification in the recommended solution
- Noisy Nets:
    - NoisyNets provided grater training stability but slowed down the training to as many as 500 Training Episodes, 3000 Learning Iterations and 12 Minutes of Execution Time thus NoisyNets are not part of our final A2C implementation

Brief Performance Overview:

For detailed discussion, see "README - Complete Analysis.md" file. Here, are the highlights:

The Impact of introducing Algorithmic Modification was to allow the agent to actually learn. Introduction of Task-Specific Modifications allowed the agent to reach the 50,000 steps per episode level.  

![github-small](https://github.com/sebtac/MLxE/blob/main/Sewak%20-%20Models%20Comparison%20-%208-Games%20MA.jpg)
 
 
The Introduction of the thread-based, iterative, synchronous MLxE architecture (ISTB), allowed the agent to achieve 50K performance level just after executing 350 Episodes.
 
![github-small](https://github.com/sebtac/MLxE/blob/main/Sewak%20Task%20Modified%20vs.%20ISTB%20Comparison%20-%208-Games%20MA.jpg)

The comparison of different MLxE architectures shows that the recommended architecture (IS-MLxE) outperforms the other architectures in the testing task on all key performance measures: overall training time, number of iterations, number of episodes.

![github-small](https://github.com/sebtac/MLxE/blob/main/MLxE%20Architectures%20Performance%20Comparison.jpg)

The initial IS-MLxE architecture was subsequently fine-tuned resulting in best performant model after additional of the following elements:
-    MLxE-IS Architecture
-    Three-Factor Priority Memory Buffer Weights
-    Minimum Model Update Steps of 64
-    Reversed e-Greedy Policy

It is important to note that implementation based on the first three items was the best performing overall. We decided to add the Reversed e-Greedy Policy as it provided additional convergence guarantees. None of the (10) runs of the model failed to converge. But one of the runs took twice as many episodes to learn as the others. Increasing that measure to 150. Using the trimmed average (where the best and the worst run was excluded) returned the performance of 140 episodes.

The graphs below compare the initial implementation of the A3C algorithms using the MLxE-IS architecture with its version after implementation of all of the above updates:

![github-small]( https://github.com/sebtac/MLxE/blob/main/A3C%20MLxE-IS%20Performance.jpg)

The final implementation of the A3C algorithm with the MLxE-IS Architecture is in file A3C MLxE-IS - PRB + MMUF + Reversed e-Greedy.py

# Next Steps

1.) Implement RAINBOW and SAC algorithms

2.) Update the A3C implementation with Distributional RL

3.) Explore further architecture enhancements especially that concerning the OA-MLxE Architecture

4.) Add complete functionality to the MLxE implementations (model saving, replay)

# How to Use MLxE

1.)    Download the files

2.)    Make sure you have installed

-    TensorFlow
-    TensorFlow-addons
-    Gym

3.)    Use the a3c_master_sewak.py file to run all basic A3C implementation including the ISTB by modifying selection at the top of the file

4.)    Use architecture specific MLxE files to run each implementation - all modules (model, memory Buffer, workers) are implemented within each of the architecture specific files. This is done so to make it easy to follow the data flow during training and to examine the interplay between various elements of the implementation.

5.)    For the tests of the individual A3C adjustment use the files with the “Development” in the name. We have kept the all debugging print() statements for ease of exploration.

6.)    For the final A3C MLxE implementation use the file: 

A3C MLxE-IS - PRB + MMUF + Reversed e-Greedy.py
