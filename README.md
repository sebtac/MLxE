# MLxE Architecture for Reinforcement Learning with Algorithmc Enchancements

# Objectives of the Project:
To intorduce a highly efficient implementation of the Reinforcement Learning (RL) algorithms that takes adventage of both the recent algorithmic developments inthe field of RL and maxium capabilities of the hardware on which the trainig is run. This efficiency is achieved via employeeing the following three proposiotions:

1.) MLxE Architecture, a highly efficient, Multiprocessing based implementation of Reinforcement Learning (RL) algorithms that maximizes the utilization of available hardware recourses. It achieves >2x efficiency improvement relative to Threading based implementations in the test environment.

2.) Update to the classical implemntations of A3C, RAINBOW and SEC algorithms with recent developments in the field of RL such as #########

3.) Addition of the "Task-at-Hand" specific modifications to the learning process to maximize the learining efficiency

# MLxE Architecture
MLxE Architecture is designed for RL learning and research applications with focus on ease of understanding and of modifying underlying RL algorithms (A3C, Rainbow, SAC,...) thus it is implemented in linear fashion with limited use of python programmatical overhead (Classes, Functions,...). 

This is an TensorFlow 2 implementation making it one of the most up to date implementations of the RL Algorithms

At its core, MLxE Architecture consist of multiple processes (called Memorizer, Learner and Executors) executed (optimally) in parallel, each with its deditacted function:

M Process - the Memorizer: Manages the Memory of the Learining Agent. It collects examples generated by Executors, modifies them and repackages them for consumption by the Learner

L Process - the Learner: The core proccess of the Learing Agent responsible for updating model weights through a SGD based algorithm. It consumes the trainig examples prepared by Memorizer and sends updated models to Executors for generation of the next wave of the examples.

xE Processes - the Executors: Generate new examples for trainig utilizing the model computed by the Learner. There are as many (x) executors as there are cores on the machine you are using thus "xE" in the name of the archtecture. This number can be reduced by 2 if the Memorizer and the Learner are imlemented in their own processes. For example, this project was created using the ML6E and ML8E architectures since my machine has 8 CPU cores. You can execute the Learner's calcualtions on GPU but still one CPU core will be dedicated to the process handing the Learner. 

There are three sub-architectures tested. We are comparing their performce and make recomendation for the best performing one: the Iterative-Synchronous MLxE:

1.) Iterative-Synchronious Threading Based (IS TB) - it is our banchark implementation based on modification of a code available online.##########
- Iterative - learning phase is perfomed intermittently with the example generation phase
- Synchronious - each executor generates only one example per iteration, waiting idle for closing of all executors and update of the model in given iteration
- Model update for the Learner and Executors is performed at the end of each iteration

2.) Iterative-Synchronious MLxE Based (IS MLxE) - the best performing architecture, used in final implementations of the RL Algorithms
- details are the same as that of the ISTB architecutre
- due to iterative nature of the architecture, all cores are used as Executors in Example Generation Phase and one process is used for combined Memorizing and Learing Phases 

3.) Iterative-Asynchronious MLxE Based (IA MLxE) - During Example Generation Phase all Executors generate examples till each Executor generates at least one Example. in other woerds, Executors respawn if they finish before all Executors generate their first example in given iteration.

4.) On-Line-Asynchronious MLxE Based (OA MLxE) - the Memorizing and Learing Phases are spawned in their own processes and are run in parallel to the Executors. The model is continously updated as its new version becomes avaialable from the Learner and Executors pick it up upon initialization of the next Example Generation

# Algorithmic Updates

this project implemented the follwing RL Models:
1.) A3C
2.) Rainbow
3.) SAC

Sunsequently, we have updated their implementations with recent methodological developments in the field of RL, mainly:
1.) ############
2.)###########
3.)#############

We dicuss the impact of such improvements on our test environment.

# "Task-at-Hand" Specific Modifications

The "Task-at-Hand" for this project is the GYM's implementation of the Cart Pole Environment. It is the simplest, and thus the fastest, environment to work with that allows for rapid feedback while you learn the existing or explore new approachses to RL. It is also a flexible environment that allows you to "raise the bar" for your agent with few code modifications. In its base imlementation, your agent is expected to keep the pole upright for 200 moves of the cart. We, instead, aim to train an agent that can do so indifinatively (in theory at least) and will learn such level of proficency on a sample of just couple houndreds of games.

In short, cart-pole-v0 enviroment requires the agent to balance a pole on a moving cart by controling the movement of the cart (left or right). The agent gets positive reward of 1 for every move of the cart as long as the pole is standing relatively upward (as measured by the angle of the pole) and by not going beyound the edges of the plane on which the cart moves (think of a table). Once either of the failure conditions is reached, the agent gets reward of 0 and the game is reset back to a random inital position. The state is represented with an array(cart's position, cart's velocity, pole's angle, pole's angular-velocity) and there are only two actions for the cart (move-left, move-right). 

One of the objectives in the development of new Reinforcement Learning algorithms is to design algorithms that can learn usefull models accross wide range of environments without modification in the set of parameters. While this is a noble objective from the theoretical perspective, it is a limiting factor in real life applications. We should not accept a model for driving a car just becouse it is also capable of flying us to the Space Station. We should also be open to modifications to the learning algorithms and/or to their implementations that with addition of only couple of lines of code will lead to singificant speedup in learning.

Another motivation for intoduction of the "Task-at-Hand" Specific Modifications is to allow the agent to learn a "proper" behaviour. In many implementations, just achieving the goal of the task is enough to call it a success. In others, the way that such goal is achieved matters as much. For example, you want your car to drive on the streets following certain road rules and not just do it as efficantly as a racing driver would. In the case of the Cart Pole environment, we have traind a lot of "successfull" agents that were able to maintain upright the pole for 10s of thousands of cart moves. But they were doing it in an "akward" way where the pole was almost not moving at all or it was stabilized close to the edges of the plane on which the cart moves. Both behaviours are "not-human-like" in a sense that we expect human to play the game trying to stabilize the pole in the middle of the plane and to control it not in a such stiff manner. Thus, we have introduced the following "Task-at-Hand" Specific Modifications:

1.) Modification of the State Representation
* Addition of the squared position (env.state[0]) - this allows the agent to learn quicker that being close to the center is good for avoiding running out of plane and helps it behave more "naturally".

2.) Advarse Modification of the Initial Position Parameters
* Addition of a randomly selected initial conditions that put the cart-pole in one of the "dengerous" positions at the begginig of the game: close to the edge of the plane or leaning to the side heavilly - the agent intializes each game by drawing random values for the four state parameters (close to 0 each). Advarse initialization additionaly modifies randomly one of those parameters. The probability of the advarse initialization decreases as trainig progresses similarly to the monotonic deacres in larning rate of the optimizer. This allows the trainig agent to experience advarse conditions more freaqently (a state with much higher informative value than other states, see the next point). 

3.) Modifying the Reward - Note that only the fail condition results in a true learning feedback (no reward). All other states return value of 1, thus the agent cannot know that leaning can be bad until it reaches that threashold angle beyound which the game ends. Thus we have introdcued two modification to the reward
* Failure "Reward" set to -10 (instead of 0) - to provide the agent with better "incentive" to avoid failure conditions. What truly matters here is a magnitude of the difference betwen the "Negative-State" and "Positive-State" Rewards. We should achive comperable results by increasing the Positive-State Reward.
* Introdcution of non-constant reward discount factor (the gamma coeffcient) - in RL, to measure how good the state we are in or the action we just took are, we are using cumulative rewards that is the reward that is a sum of the imidiate reward we get from being in given state and/or taking given action in that state and the expected sum of future rewards we get following such move. Note that in environments with infite horizons (i.e. where the game can last indifinativly and cart-pole is such!) the expected cumulative reward can be infinite as well. Infinities are not friendly to RL algorithms thus, in practice, we use a discount factor (DF) in front of the Expected Sum of Future Rewards term to limit this value. the DF value is between 0-1, usually close to 1 like .99,.95 or.90. The selection of that value has also practical/conceptual meaning as it defines how much do we care about the potential rewards that we can achive far in the future. The closer the value of DF is to 1 the further ahead we attribute the effects of the current actions. Our attention span increases in a way. We theoretize that in the case of the cartpole attention span of the agent should differ depending on the "saftey" of the state the agent is in. If it is relatively safe state you the agent can think about returns in a long time span. If it is a relaively dengorous state the agent should focus more on imidiate consequences of its actions. Thus we have intorduced the algorithm that modifies the value of the discount factor depending on how cloese given state is to the failure condition.   



