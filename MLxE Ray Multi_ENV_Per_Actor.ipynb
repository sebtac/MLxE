{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df910e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### DRAFT ##################\n",
    "Implementation of GYM's Multi_ENV_Per_Actor parameter with MLxE architecture.\n",
    "\n",
    "- Implementation with RAY works\n",
    "- To finalize the work VECTORIZED GYM ENVIRONMENTS needs custom RESET method to allow proper RESET at the end of the episode\n",
    "\n",
    "See StackOverflow issue:\n",
    "    https://stackoverflow.com/questions/75551863/vectorized-gym-environments-how-to-block-automatic-environment-reset-on-done-t\n",
    "        \n",
    "Possibel ways to fix it:\n",
    "    - change the guts of the GYM package.\n",
    "    - checkout gym3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import ray, gym, time, math\n",
    "ray.RAY_memory_monitor_refresh_ms = 0 # Should stop providing memory warinings!!! but it does not !!!! \n",
    "\n",
    "from ray.util.queue import Queue\n",
    "from ray._private.utils import get_num_cpus\n",
    "\n",
    "cores = get_num_cpus()\n",
    "collect_obs = Queue()\n",
    "collect_examples = Queue()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa \n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.20f}\".format(x)})\n",
    "np.set_printoptions(precision=4, floatmode=\"maxprec\",suppress=True, linewidth=200)\n",
    "\n",
    "args = {# SYSTEM Settings\n",
    "        \"executors_n\": 1, #get_num_cpus(), # How many Executors are running in parallel\n",
    "\n",
    "        # GYM Settings\n",
    "        \"max_episodes\": 500, # 500 How many Episodes do you want to run?\n",
    "        \"num_envs_per_worker\": 10, # how many environments to run per worker -- tested till 512!!!\n",
    "        \"env_name\": 'CartPole-v1', # the name of the GYM Environment to run\n",
    "\n",
    "        \n",
    "        # MODEL Definition\n",
    "        \"state_n\": 4, # value of env.observation_space() (*2 as current_state and next_state)\n",
    "        \"state_n_adj\": 1, # all Task Specific Adjustments (*2 as current_state and next_state)\n",
    "        \"state_n_add\": 4, # all additional step descriptions (Reward, Done, Action, Discounted Reward)\n",
    "        \"action_n\": 2, # value of env.action_space()\n",
    "        \"common_layers_n\": [128,256,128], # Number of Neurons in Common Layers of A3C Model\n",
    "        \"value_layers_n\": [64,128,64], # Number of Neurons in Value Layers of A3C Model\n",
    "        \"policy_layers_n\": [64,128,64], # Number of Neurons in Policy Layers of A3C Model\n",
    "         \n",
    "        # LEARNER Settings\n",
    "        \"batch_size\": 128, # Number of examples per model update\n",
    "        \"model_alignment_frequency\": 128, # frequency of synchronization of the Target Model with On-Line Model -- OPTIONAL\n",
    "        \"minimum_model_update_frequency\": 64, # How Many Model Updates to run in each iteration at minimum.\n",
    "        \"epsilon_decay_policy\":1, # Run EDP? 0 No, 1 Yes\n",
    "        \"epsilon\": 0.1, # Original 0.1                 # e-greedy when exploring\n",
    "        \"epsilon_decay\": 0.995, # Original 0.995         # epsilon decay r\n",
    "        \n",
    "        # LEARNING RATE Decceleration\n",
    "        \"lr_alpha\": 0.0001, # Inital LR\n",
    "        \"lr_alpha_power\": 0.998, #0.998 bast so far # Controls the pace of LR depreciation\n",
    "        \"lr_alpha_limit\": 0.000001, # Lower limit on the LR\n",
    "        \n",
    "        # EXECUTOR Settings\n",
    "        \"internal_step_counter_limit\": 50000, # 500000 limit of steps per episode \n",
    "        \"experience_batch_size\": 1024, # how many steps to save into the memory buffer from each run\n",
    "        \"experience_max_batch_size\": 256, # Maximum number of cases to save to Memory Baffer from each experience (most recent) (tested 1024-64: 256 Best)\n",
    "        \n",
    "        # advarse STATE Probability\n",
    "        \"prob_advarse_state_initial\": 0.2, # Probability of choosing advarse case when starting new episode\n",
    "        \"prob_advarse_state_type_multiplier\": 0.0, #Adjustment to the Probability to control the ratio of issue types that are promoted through advarse initialization\n",
    "        \n",
    "        # REWARD Incentives\n",
    "        \"reward_negative\": -10.0, #-10.0, # Override the GYM's default negative return\n",
    "        \n",
    "        # PRIORITY MEMORY BUFFER\n",
    "        \"pmb_cols\": 5,  #\n",
    "        \"pmb_alpha\": 0.9, # Not Implemented                          # priority parameter, alpha=[0, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "        \"pmb_beta\": 0.4, # Tested till 0.8 but best reuslts with 1.0 # importance sampling parameter, beta=[0, 0.4, 0.5, 0.6, 1]\n",
    "        \"pmb_beta_increment\": 0.001, # Originally 0.001\n",
    "        \"pmb_td_error_margin\": 0.01,                                # pi = |td_error| + margin\n",
    "        \"pmb_abs_td_error_upper\": 1,\n",
    "               \n",
    "        }\n",
    "\n",
    "@ray.remote\n",
    "class Learner:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        \n",
    "        self.collect_examples = []     \n",
    "        \n",
    "        \n",
    "        # Define BASE Model - Target\n",
    "        self.inputs_base = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "\n",
    "        self.common_network_base = Dense(self.common_layers_n[0], activation='relu',name=\"1\")(self.inputs_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[1], activation='relu',name=\"2\")(self.common_network_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[2], activation='relu',name=\"3\")(self.common_network_base)\n",
    "\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[0], activation='relu',name=\"7\")(self.common_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[1], activation='relu',name=\"8\")(self.policy_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[2], activation='relu',name=\"9\")(self.policy_network_base)\n",
    "\n",
    "        self.value_network_base = Dense(self.value_layers_n[0], activation='relu',name=\"4\")(self.common_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[1], activation='relu',name=\"5\")(self.value_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[2], activation='relu',name=\"6\")(self.value_network_base)\n",
    "\n",
    "        self.values_base = Dense(1,name=\"10\")(self.value_network_base)            \n",
    "        self.logits_base = Dense(self.action_n,name=\"11\")(self.policy_network_base)\n",
    "\n",
    "        self.model_base = Model(inputs=self.inputs_base, outputs=[self.values_base, self.logits_base])\n",
    "\n",
    "\n",
    "        # Define MAIN Model - Trainable Model\n",
    "        self.inputs_main = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "        self.common_network_main = Dense(self.common_layers_n[0], activation='relu')(self.inputs_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[1], activation='relu')(self.common_network_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[2], activation='relu')(self.common_network_main)\n",
    "\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_main)\n",
    "\n",
    "        self.value_network_main = Dense(self.value_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[1], activation='relu')(self.value_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[2], activation='relu')(self.value_network_main)\n",
    "\n",
    "        self.values_main = Dense(1)(self.value_network_main)\n",
    "        self.logits_main = Dense(self.action_n)(self.policy_network_main)\n",
    "        \n",
    "        self.model_main = Model(inputs=self.inputs_main, outputs=[self.values_main, self.logits_main])\n",
    "\n",
    "        # Define Optimizer\n",
    "        self.optimizer = tfa.optimizers.RectifiedAdam(self.lr_alpha)\n",
    "        \n",
    "        self.executor_model = self.model_main.get_weights()\n",
    "\n",
    "        #if counter_learninig % model_alignment_frequency == 0:\n",
    "        self.model_base.set_weights(self.model_main.get_weights())    \n",
    "\n",
    "        #executor_model.append(model_main.get_weights()) # the first call MUST be append to create the entry [0]\n",
    "        #print(\"Saved Model\", worker, len(executor_model))\n",
    "\n",
    "        self.memory_buffer = np.full((self.state_n+self.state_n_adj) * 2 + self.state_n_add + self.pmb_cols,0.0)\n",
    "        self.memory_buffer = []        \n",
    "\n",
    "        # GLOBAL COUNTERS -- possibly tobe moved to a separate worker\n",
    "        self.counter_learninig = 0\n",
    "        self.episode_counter = 0\n",
    "        self.executor_counter = 0        \n",
    "        self.steps_counter = 0\n",
    "        self.internal_step_counter_best = 0\n",
    "        \n",
    "    def get_executor_model(self):\n",
    "        return self.executor_model\n",
    "\n",
    "    def get_base_model_weights(self):\n",
    "        return self.model_base.get_weights()\n",
    "\n",
    "    def get_main_model_weights(self):\n",
    "        return self.model_main.get_weights()\n",
    "    \n",
    "    def increase_counter_learninig(self):\n",
    "        self.counter_learninig += 1\n",
    "\n",
    "    def increase_episode_counter(self):\n",
    "        self.episode_counter += 1\n",
    "\n",
    "    def increase_executor_counter(self):\n",
    "        self.executor_counter += 1\n",
    "        \n",
    "    def increase_steps_counter(self, value):\n",
    "        self.steps_counter += value\n",
    "\n",
    "    def set_internal_step_counter_best(self, value):\n",
    "        self.internal_step_counter_best = value\n",
    "    \n",
    "    def reset_counter_learninig(self):\n",
    "        self.counter_learninig = 0\n",
    "\n",
    "    def reset_episode_counter(self):\n",
    "        self.episode_counter = 0\n",
    "        \n",
    "    def reset_executor_counter(self):\n",
    "        self.executor_counter = 0        \n",
    "        \n",
    "    def reset_steps_counter(self):\n",
    "        self.steps_counter = 0\n",
    "        \n",
    "    def get_counter_learninig(self):\n",
    "        return self.counter_learninig\n",
    "\n",
    "    def get_episode_counter(self):\n",
    "        return self.episode_counter\n",
    "        \n",
    "    def get_executor_counter(self):\n",
    "        return self.executor_counter\n",
    "    \n",
    "    def get_steps_counter(self):\n",
    "        return self.steps_counter\n",
    "\n",
    "    def get_internal_step_counter_best(self):\n",
    "        return self.internal_step_counter_best\n",
    "\n",
    "    def train(self):\n",
    "        #print(\"LEARNER.train\")\n",
    "        #print(\"LEARNER RUN\", self.episode_counter)\n",
    "\n",
    "        # Adjust Monotonically Decreasing Learning Rate\n",
    "        self.next_lr_alpha = self.lr_alpha * np.power(self.lr_alpha_power, self.episode_counter)\n",
    "        if self.next_lr_alpha < self.lr_alpha_limit:\n",
    "            self.next_lr_alpha = self.lr_alpha_limit\n",
    "\n",
    "        self.optimizer.learning_rate = self.next_lr_alpha\n",
    "        \n",
    "        #print(\"next_lr_alpha\", self.next_lr_alpha)\n",
    "\n",
    "        # Initialize LEARNER\n",
    "        #while self.collect_examples.qsize() > 0: # and episod_counter.value < max_episodes:\n",
    "\n",
    "        #print(\"LEARNER len(self.collect_examples)\", len(self.collect_examples))\n",
    "        for i in self.memory_buffer:\n",
    "\n",
    "            self.example = i\n",
    "            #print(\"LEARNER self.example\", self.example.shape)\n",
    "            \n",
    "            #print(\"WEIGHTS BASE\", self.model_base.get_weights()[0][0][0])\n",
    "            #print(\"WEIGHTS MAIN\", self.model_main.get_weights()[0][0][0])\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                #print(\"CS\", example[:,:5])\n",
    "                self.values, self.logits = self.model_base(tf.convert_to_tensor(self.example[:,:5], dtype=tf.float32))\n",
    "                #print(\"Disc Reward\", example[:,-(pmb_cols+1)])\n",
    "                self.advantage = tf.convert_to_tensor(np.expand_dims(self.example[:,-(self.pmb_cols+1)],axis=1), dtype=tf.float32) - self.values\n",
    "                self.value_loss = self.advantage ** 2 # this is a term to be minimized in trainig \n",
    "                self.policy = tf.nn.softmax(self.logits)\n",
    "                self.entropy = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(labels=self.policy, logits=self.logits), [-1,1])\n",
    "                #print(\"Action\", example[:,-(pmb_cols+2)])\n",
    "                self.policy_loss = tf.reshape(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=list(self.example[:,-(self.pmb_cols+2)].astype(int)), logits=self.logits), [-1,1])            \n",
    "                self.policy_loss *= tf.stop_gradient(self.advantage) # advantage will be exluded from computation of the gradient; thsi allows to treat the values as constants\n",
    "                self.policy_loss -= 0.01 * self.entropy # entropy adjustment for better exploration \n",
    "                self.total_loss = tf.reduce_mean((0.5 * self.value_loss + self.policy_loss))\n",
    "\n",
    "\n",
    "            self.grads = tape.gradient(self.total_loss, self.model_base.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(self.grads, self.model_main.trainable_weights))\n",
    "            \n",
    "            self.counter_learninig += 1\n",
    "\n",
    "            self.model_base.set_weights(self.model_main.get_weights()) ### the THREADED IMPLEMENTATION IS SYNCHRONIZED AT EACH STEP!!!\n",
    "\n",
    "        self.executor_model = self.model_main.get_weights()\n",
    "        \n",
    "        #print(\"LEARNER T BASE\", self.model_base.get_weights()[0][0][:5])\n",
    "        #print(\"LEARNER T MAIN\", self.model_main.get_weights()[0][0][:5])\n",
    "        \n",
    "        #print(type(self.executor_model))\n",
    "        #print(\"LEARNER T EXECUTOR\", self.executor_model[0][0][:5])        \n",
    "\n",
    "        #if counter_learninig % model_alignment_frequency == 0:\n",
    "        self.model_base.set_weights(self.model_main.get_weights())\n",
    "\n",
    "        print(\"LEARINING ITERATION:\", self.counter_learninig,\"\\n\")\n",
    "        \n",
    "        #self.steps_counter = 0\n",
    "        self.executor_counter = 0\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def update_memory_buffer(self, experience):\n",
    "        \n",
    "        #self.memory_buffer.append(experience)\n",
    "        self.memory_buffer = experience\n",
    "        #print(\"LEARNER UPDATE MB\", len(self.memory_buffer))\n",
    "        \n",
    "@ray.remote\n",
    "class Memorizer:\n",
    "    def __init__(self, learner):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        \n",
    "        self.memory_buffer = np.full((self.state_n + self.state_n_adj) * 2 + self.state_n_add + self.pmb_cols,0.0)\n",
    "        self.memory_buffer[-1] = 99 \n",
    "        self.collect_obs = []\n",
    "        self.worker = \"Memorizer\"\n",
    "        \n",
    "        # Define BASE Model - Target\n",
    "        self.inputs_base = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "\n",
    "        self.common_network_base = Dense(self.common_layers_n[0], activation='relu',name=\"1\")(self.inputs_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[1], activation='relu',name=\"2\")(self.common_network_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[2], activation='relu',name=\"3\")(self.common_network_base)\n",
    "\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[0], activation='relu',name=\"7\")(self.common_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[1], activation='relu',name=\"8\")(self.policy_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[2], activation='relu',name=\"9\")(self.policy_network_base)\n",
    "\n",
    "        self.value_network_base = Dense(self.value_layers_n[0], activation='relu',name=\"4\")(self.common_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[1], activation='relu',name=\"5\")(self.value_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[2], activation='relu',name=\"6\")(self.value_network_base)\n",
    "\n",
    "        self.values_base = Dense(1,name=\"10\")(self.value_network_base)            \n",
    "        self.logits_base = Dense(self.action_n,name=\"11\")(self.policy_network_base)\n",
    "\n",
    "        self.model_base = Model(inputs=self.inputs_base, outputs=[self.values_base, self.logits_base])   \n",
    "        self.base_model_weights = ray.get(learner.get_base_model_weights.remote())\n",
    "        self.model_base.set_weights(self.base_model_weights)\n",
    "        \n",
    "        # Define MAIN Model - Trainable Model\n",
    "        self.inputs_main = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "        self.common_network_main = Dense(self.common_layers_n[0], activation='relu')(self.inputs_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[1], activation='relu')(self.common_network_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[2], activation='relu')(self.common_network_main)\n",
    "\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_main)\n",
    "\n",
    "        self.value_network_main = Dense(self.value_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[1], activation='relu')(self.value_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[2], activation='relu')(self.value_network_main)\n",
    "\n",
    "        self.values_main = Dense(1)(self.value_network_main)\n",
    "        self.logits_main = Dense(self.action_n)(self.policy_network_main)\n",
    "        \n",
    "        self.model_main = Model(inputs=self.inputs_main, outputs=[self.values_main, self.logits_main])\n",
    "        self.main_model_weights = ray.get(learner.get_main_model_weights.remote())\n",
    "        self.model_main.set_weights(self.main_model_weights) \n",
    "        \n",
    "        #self.experience_length_all = np.array([]).reshape(0, self.executors_n)\n",
    "        self.experience_length_all = []\n",
    "        self.experience_length = []\n",
    "        \n",
    "    \n",
    "    def get_experience_length_all(self):\n",
    "        return self.experience_length_all[1:]\n",
    "    \n",
    "    def get_collected_obs(self):\n",
    "        return self.collect_obs\n",
    "        \n",
    "    def collect(self, learner, experience):\n",
    "        #print(\"MEMORIZER.collect\")\n",
    "        \n",
    "        #print(\"MEMORIZER.collect - len(experience)\", len(experience), \"experience\")\n",
    "        self.collect_obs.append(experience)\n",
    "        \n",
    "        #print(\"MEMORIZER - EXECUTORS\", ray.get(learner.get_executor_counter.remote()))\n",
    "        \n",
    "        if ray.get(learner.get_executor_counter.remote()) < self.executors_n:\n",
    "            #print(\"WAITING IN MEMORIZER 1\", ray.get(learner.get_executor_counter.remote()), ray.get(learner.get_steps_counter.remote()), ray.get(learner.get_episode_counter.remote()))\n",
    "            pass\n",
    "        else:\n",
    "            #print(\"WAITING IN MEMORIZER 2\")\n",
    "            #print(\"MEMORY UPDATE\", self.collect_obs)\n",
    "            self.memory_update(learner, self.collect_obs)\n",
    "            #ray.get(learner.update_memory_buffer.remote(self.memory_buffer))\n",
    "            \n",
    "    def collect_length(self, experience_length):\n",
    "        self.experience_length.append(experience_length)\n",
    "        #print(\"MEMORIZER.collect_length\")\n",
    "            \n",
    "    #def memory_update(self, learner, collect_obs, collect_examples):\n",
    "    def memory_update(self, learner, collect_obs):\n",
    "        #print(\"MEMORIZER.memory_update\")\n",
    "        \n",
    "        while ray.get(learner.get_executor_counter.remote()) < self.executors_n:\n",
    "            pass\n",
    "        \n",
    "        self.base_model_weights = ray.get(learner.get_base_model_weights.remote())\n",
    "        self.model_base.set_weights(self.base_model_weights)\n",
    "        \n",
    "        self.main_model_weights = ray.get(learner.get_main_model_weights.remote())\n",
    "        self.model_main.set_weights(self.main_model_weights)\n",
    "        \n",
    "        #print(\"MEMORIZER MU BASE\", self.model_base.get_weights()[0][0][:5])\n",
    "        #print(\"MEMORIZER MU MAIN\", self.model_main.get_weights()[0][0][:5])\n",
    "        \n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        #self.experience_length = np.zeros((1,self.executors_n))\n",
    "        #self.experience_length = []\n",
    "        \n",
    "        #print(\"BEFORE\", len(self.experience_length))\n",
    "        \n",
    "        #print(\"collect_obs\", len(collect_obs[0]), \"collect_obs\")\n",
    "        #print(\"BEFORE MEMORY BUFFER\", self.memory_buffer.shape, self.memory_buffer)\n",
    "        for i in collect_obs[0]:\n",
    "            self.mem_counter += 1\n",
    "            self.exp_temp = i\n",
    "            \n",
    "            #print(\"self.exp_temp\", self.exp_temp.shape, \"self.exp_temp\", \"self.exp_temp\")\n",
    "            \n",
    "            #self.experience_length[0,self.mem_counter] = self.exp_temp.shape[0]\n",
    "            #self.experience_length.append(self.exp_temp.shape[0])\n",
    "\n",
    "            #print(self.worker, \"self.exp_temp\", type(self.exp_temp), self.exp_temp)\n",
    "\n",
    "            #print(self.worker, mem_counter, \"memory_buffer B_STACK\", self.memory_buffer.shape)\n",
    "            #print(self.worker, mem_counter, \"exp_temp B_STACK\", self.exp_temp.shape)\n",
    "\n",
    "            self.memory_buffer = np.vstack((self.memory_buffer, self.exp_temp))\n",
    "            #print(self.worker, self.mem_counter, \"memory_buffer A_STACK\", self.memory_buffer.shape)\n",
    "            #print(self.worker, mem_counter, \"MIN INPUTS\", self.memory_buffer.shape[0], self.experience_batch_size * self.executors_n) \n",
    "            \n",
    "            self.memory_buffer = self.memory_buffer[-np.minimum(self.memory_buffer.shape[0], self.experience_batch_size * self.executors_n):,:]\n",
    "\n",
    "            #print(self.worker, self.mem_counter, \"memory_buffer A_MIN\", self.memory_buffer.shape)\n",
    "        \n",
    "        #self.experience_length_all = np.vstack([self.experience_length_all, self.experience_length])\n",
    "        \n",
    "        if self.memory_buffer[0,-1] == 99:\n",
    "            self.memory_buffer = self.memory_buffer[1:,:]\n",
    "            \n",
    "        #print(\"AFTER MEMORY BUFFER\", self.memory_buffer.shape, self.memory_buffer)\n",
    "        #print(\"AFTER\", len(self.experience_length))\n",
    "        \n",
    "        self.experience_length_all.append(self.experience_length)\n",
    "        \n",
    "        # PRIORITY MEMORY BUFFER\n",
    "\n",
    "        # Inverse Discounted Reward Probability\n",
    "        self.dr_min = np.min(self.memory_buffer[:,-(self.pmb_cols+1)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = self.memory_buffer[:,-(self.pmb_cols+1)] - self.dr_min\n",
    "        self.dr_max = np.max(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = 1 - self.memory_buffer[:,-(self.pmb_cols)] / self.dr_max + 0.01\n",
    "        self.dr_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = self.memory_buffer[:,-(self.pmb_cols)] / self.dr_sum\n",
    "\n",
    "        # Inverse \"Age\" Probability\n",
    "        self.memory_buffer[:,-(self.pmb_cols-1)] += 1\n",
    "        self.age_max = np.max(self.memory_buffer[:,-(self.pmb_cols-1)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-2)] = self.age_max - self.memory_buffer[:,-(self.pmb_cols-1)] + 1.0\n",
    "        self.age_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols-2)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-2)] = self.memory_buffer[:,-(self.pmb_cols-2)] / self.age_sum\n",
    "\n",
    "        # Proportional TD Error Probability                \n",
    "        self.target_q, self.target_logits = self.model_base(tf.convert_to_tensor(self.memory_buffer[:,5:10], dtype=tf.float32))\n",
    "        self.td_target = np.expand_dims(self.memory_buffer[:,10],axis = -1) + 0.9 * self.target_q * np.expand_dims((1 - self.memory_buffer[:,11]),axis = -1)\n",
    "        self.predict_q, self.predict_logits = self.model_main(tf.convert_to_tensor(self.memory_buffer[:,:5], dtype=tf.float32))\n",
    "        self.abs_td_error = np.abs(self.td_target - self.predict_q) + self.pmb_td_error_margin\n",
    "        self.clipped_td_error = np.where(self.abs_td_error < self.pmb_abs_td_error_upper, self.abs_td_error, self.pmb_abs_td_error_upper)                \n",
    "        self.memory_buffer[:,-(self.pmb_cols-3)] = self.clipped_td_error[:,0]\n",
    "        self.td_error_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols-3)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-3)] = self.memory_buffer[:,-(self.pmb_cols-3)] / self.td_error_sum\n",
    "\n",
    "        self.memory_buffer[:,-1] = np.average(self.memory_buffer[:,[-(self.pmb_cols-2),-(self.pmb_cols-3)]], axis = 1 ) # Best! 280-310\n",
    "        self.pmb_beta = min(1., self.pmb_beta + self.pmb_beta_increment * self.executors_n)\n",
    "        self.memory_buffer[:,-1] = np.power(self.memory_buffer[:,-1], self.pmb_beta)\n",
    "        self.total_error_sum = np.sum(self.memory_buffer[:,-1])\n",
    "        self.memory_buffer[:,-1] = self.memory_buffer[:,-1] / self.total_error_sum\n",
    "\n",
    "        self.prob_sum_check1 = np.sum(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.prob_sum_check2 = np.sum(self.memory_buffer[:,-(self.pmb_cols-2)])\n",
    "        self.prob_sum_check3 = np.sum(self.memory_buffer[:,-(self.pmb_cols-3)])\n",
    "        self.prob_sum_check = np.sum(self.memory_buffer[:,-1])\n",
    "\n",
    "        self.batch_size_min = np.minimum(self.batch_size,self.memory_buffer.shape[0])\n",
    "        self.runs = self.memory_buffer.shape[0] // np.minimum(self.memory_buffer.shape[0], self.batch_size_min) + 1\n",
    "        self.runs = np.maximum(self.minimum_model_update_frequency, self.runs)\n",
    "        \n",
    "        collect_samples = []\n",
    "        \n",
    "        for i in range(self.runs):\n",
    "\n",
    "            self.sample_index = np.random.choice(self.memory_buffer.shape[0],\n",
    "                                            np.minimum(self.memory_buffer.shape[0], self.batch_size_min),\n",
    "                                            p = self.memory_buffer[:,-1],\n",
    "                                            replace=False)\n",
    "            \n",
    "            self.sample = self.memory_buffer[self.sample_index, :]\n",
    "            \n",
    "            #print(\"self.sample\", self.sample.shape, \"self.sample\")\n",
    "\n",
    "            collect_samples.append(self.sample)\n",
    "        \n",
    "        #print(\"self.sample\", self.sample)\n",
    "        \n",
    "        self.collect_obs = []\n",
    "        self.experience_length = []\n",
    "        \n",
    "        #print(\"MEMORIZER len(collect_examples)\", len(collect_examples))\n",
    "        ray.get(learner.update_memory_buffer.remote(collect_samples))\n",
    "        #print(\"MEMORIZER Before TRAIN\")\n",
    "        ray.get(learner.train.remote())\n",
    "        #print(\"MEMORIZER After TRAIN\")\n",
    "    \n",
    "@ray.remote\n",
    "class Executor:\n",
    "    #def __init__(self, memorizer, learner):\n",
    "    def __init__(self, i, args):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]/16\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        self.num_envs_per_worker = args[\"num_envs_per_worker\"]\n",
    "        \n",
    "        self.collect_examples = collect_examples\n",
    "        \n",
    "        # Establish Environment\n",
    "        #self.env = gym.make(self.env_name).unwrapped #unwrapped to access the behind the scenes elements of the environment\n",
    "        #self.env = gym.vector.make(self.env_name, num_envs=3).unwrapped #unwrapped to access the behind the scenes elements of the environment\n",
    "        \n",
    "        self.env = gym.vector.SyncVectorEnv([lambda: gym.make(self.env_name).env for _ in range(self.num_envs_per_worker)])\n",
    "        #self.env = gym.vector.SyncVectorEnv([lambda: gym.make(self.env_name) for _ in range(self.num_envs_per_worker)])\n",
    "        #self.env = gym.vector.AsyncVectorEnv([lambda: gym.make(self.env_name).env for _ in range(self.num_envs_per_worker)])\n",
    "        \n",
    "        #print(\"self.env.state\", self.env.state)\n",
    "        #print(\"self.env.state\", self.env.observations)\n",
    "        \n",
    "        #self.env = gym.vector.SyncVectorEnv([\n",
    "        #    lambda: gym.make(self.env_name),\n",
    "        #    lambda: gym.make(self.env_name),            \n",
    "        #    lambda: gym.make(self.env_name)])\n",
    "        \n",
    "        # Define A3C Model for Executors\n",
    "        #self.inputs_executor = tf.keras.Input(shape=(self.num_envs_per_worker,self.state_n + self.state_n_adj,))\n",
    "        self.inputs_executor = tf.keras.Input(shape=(self.state_n + self.state_n_adj,))\n",
    "        \n",
    "        self.common_network_executor = Dense(self.common_layers_n[0], activation='relu')(self.inputs_executor)\n",
    "        self.common_network_executor = Dense(self.common_layers_n[1], activation='relu')(self.common_network_executor)\n",
    "        self.common_network_executor = Dense(self.common_layers_n[2], activation='relu')(self.common_network_executor)\n",
    " \n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_executor)\n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_executor)\n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_executor)\n",
    "        \n",
    "        self.value_network_executor = Dense(self.value_layers_n[0], activation='relu')(self.common_network_executor)\n",
    "        self.value_network_executor = Dense(self.value_layers_n[1], activation='relu')(self.value_network_executor)\n",
    "        self.value_network_executor = Dense(self.value_layers_n[2], activation='relu')(self.value_network_executor)\n",
    "        \n",
    "        self.logits_executor = Dense(self.action_n)(self.policy_network_executor)\n",
    "        self.values_executor = Dense(1)(self.value_network_executor)\n",
    "        \n",
    "        self.model_executor = Model(inputs=self.inputs_executor, outputs=[self.values_executor, self.logits_executor])\n",
    "        \n",
    "        self.observations = []\n",
    "        self.iter_counter = 0\n",
    "        \n",
    "        self.internal_step_counter_all = 0\n",
    "        self.done_indeed = 0\n",
    "        self.done_done_all = np.zeros(3)\n",
    "        \n",
    "        \n",
    "    def experience_generator(self, i, learner, memorizer):\n",
    "        #def experience_generator(self, i, learner):        \n",
    "        \n",
    "        #print(\"EG BEOFRE WHILE\", i)\n",
    "        \n",
    "        self.worker = i\n",
    "        self.time_start = time.time()\n",
    "        \n",
    "        reload_model_weights = 1\n",
    "        \n",
    "        #while (ray.get(learner.get_episode_counter.remote()) < self.max_episodes) and (ray.get(learner.get_internal_step_counter_best.remote()) < self.internal_step_counter_limit):\n",
    "        #print(\"BEFORE FIRST WHILE\", ray.get(learner.get_episode_counter.remote()), self.max_episodes, self.iter_counter, self.internal_step_counter_limit)\n",
    "        while (ray.get(learner.get_episode_counter.remote()) < self.max_episodes) and (self.iter_counter < self.internal_step_counter_limit):\n",
    "            #print(\"AFTER FIRST WHILE\", ray.get(learner.get_episode_counter.remote()), self.max_episodes, self.iter_counter, self.internal_step_counter_limit)\n",
    "            #print(\"EG AFTER FIRST WHILE\", i)\n",
    "            \n",
    "            self.iter_counter += 1\n",
    "            \n",
    "            #print(\"reload_model_weights\", reload_model_weights)\n",
    "            \n",
    "            if reload_model_weights == 1:\n",
    "                self.model_executor.set_weights(ray.get(learner.get_executor_model.remote()))\n",
    "                reload_model_weights = 0\n",
    "            \n",
    "            #print(\"EG\", self.model_executor.get_weights()[0][0][:5])\n",
    "                \n",
    "            # Collect Examples & Save them in the Central Observation Repository\n",
    "            self.current_state = self.env.reset()\n",
    "            \n",
    "            #print(i, \"STATE AFTER INITIAL RESET\", self.current_state)\n",
    "            \n",
    "            \n",
    "            #print(\"EPISODE CONTER\", ray.get(learner.get_episode_counter.remote()))\n",
    "            # ENSURE EXPLORATION OF advarse STATES\n",
    "            if ray.get(learner.get_episode_counter.remote()) <= 1:\n",
    "                self.prob_advarse_state = self.prob_advarse_state_initial\n",
    "            else:\n",
    "                self.prob_advarse_state = np.clip(self.prob_advarse_state_initial/math.log(ray.get(learner.get_episode_counter.remote()),5), 0.05, 0.2)\n",
    "            \n",
    "            self.prob_random_state = 1-self.prob_advarse_state*4\n",
    "            \n",
    "            # CartPole position_start:\n",
    "            # 0: Close to the Left Edge\n",
    "            # 1: Close to the Right Edge\n",
    "            # 2: Normal, random start (env.restart())\n",
    "            # 3: Leaning Heavilly to the Left\n",
    "            # 4: Leaning Heavilly to the Right\n",
    "            \n",
    "            #print(i, \"self.current_state BEFORE\", self.current_state)\n",
    "            \n",
    "            # Choose one of the 5 scenarios with probabilities defined in p=()\n",
    "            self.pos_start = np.random.choice(5,\n",
    "                                              size = self.num_envs_per_worker, \n",
    "                                              p = (self.prob_advarse_state + self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_advarse_state + self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_random_state,\n",
    "                                              self.prob_advarse_state - self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_advarse_state - self.prob_advarse_state_type_multiplier * self.prob_advarse_state))\n",
    "            \n",
    "            #print(\"self.pos_start\", self.pos_start)\n",
    "            \n",
    "            rows = np.where((self.pos_start == 0) | (self.pos_start == 5))\n",
    "            self.current_state[rows,0] = -1.5 # -2.4 MIN\n",
    "            rows = np.where(self.pos_start == 1)\n",
    "            self.current_state[rows,0] = 1.5 # 2.4 MAX\n",
    "            rows = np.where(self.pos_start == 3)\n",
    "            self.current_state[rows,2] = -0.150 #-0.0.20943951023931953 MIN\n",
    "            rows = np.where(self.pos_start == 4)            \n",
    "            self.current_state[rows,2] = 0.150 #0.0.20943951023931953 MAX            \n",
    "\n",
    "            #print(i, \"self.current_state\", self.current_state)\n",
    "            \n",
    "            #self.env.state = self.current_state\n",
    "            \n",
    "            #print(i, \"self.current_state UPDATED\", self.current_state)\n",
    "            \n",
    "            self.env_id = 0\n",
    "            #print(dir(self.env))\n",
    "            for self.env_indivdual in self.env.envs:\n",
    "                #print(self.env_indivdual.state)\n",
    "                self.env_indivdual.state = self.current_state[self.env_id, :]\n",
    "                #print(self.env_indivdual.state)\n",
    "                self.env_id += 1\n",
    "            #self.env.observations = self.current_state\n",
    "            #self.env.blablabla = self.current_state\n",
    "            \n",
    "            \n",
    "            #print(i, \"self.current_state AFTER\", self.current_state)\n",
    "            #print(i, \"self.env.state\", self.env.state)\n",
    "            #print(i, \"self.env.observatons AFTER\", self.env.observations)\n",
    "            #print(i, \"self.env.blablabla AFTER\", self.env.blablabla)            \n",
    "            \n",
    "            # Custom State Representation Adjustment to help agent learn to be closer to the center\n",
    "            self.current_state = np.append(self.current_state,(self.current_state[:,0] * self.current_state[:,0]).reshape((-1,1)), axis = 1) \n",
    "            \n",
    "            #print(i, \"STATE AFER APPEND\", self.current_state) \n",
    "            \n",
    "            self.observations = np.empty((1,(self.state_n + self.state_n_adj) * 2 + 3, self.num_envs_per_worker))\n",
    "            self.done = np.array([False for i in range(self.num_envs_per_worker)])\n",
    "            self.internal_step_counter = 0\n",
    "            self.collect_obs = []\n",
    "            self.collect_obs_length = []\n",
    "            self.done_indeed = 0\n",
    "            #print(i, \"self.observations INITIAL SHAPE\", self.observations.shape)\n",
    "            \n",
    "            #print(\"self.done BEFORE\", self.done)\n",
    "            #print(i, \"BEFORE SECOND WHILE\", self.done_indeed == 0, self.internal_step_counter <= self.internal_step_counter_limit)\n",
    "            #while (not(self.done.all() == True)) and (self.internal_step_counter <= self.internal_step_counter_limit):\n",
    "            self.done_ind = 0\n",
    "            self.done_ind_new = 0\n",
    "            self.done_done_all = 0\n",
    "            self.internal_step_counter_joint = []\n",
    "            \n",
    "            while (self.done_indeed == 0) and (self.internal_step_counter <= self.internal_step_counter_limit):                \n",
    "                #print(i, \"WITHIN SECOND WHILE\", self.done, self.done_indeed == 0, self.internal_step_counter <= self.internal_step_counter_limit)\n",
    "                #print(i, \"self.current_state 2\", self.current_state, self.current_state.shape)\n",
    "                #self.values, self.logits = self.model_executor(tf.convert_to_tensor(np.array(np.expand_dims(self.current_state,axis=0)), dtype=tf.float32))\n",
    "                self.values, self.logits = self.model_executor(tf.convert_to_tensor(np.array(self.current_state), dtype=tf.float32))\n",
    "                \n",
    "                # EPSILON-GREEDY with DECAY POLICY\n",
    "                \n",
    "                if self.epsilon_decay_policy == 1:\n",
    "                    #epsilon *= (2-epsilon_decay)\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    if self.epsilon >= np.random.rand(): # Random-Informed\n",
    "                        #print(\"self.logits\", self.logits)\n",
    "                        #self.action = np.argmax(self.logits, axis = 2).squeeze()\n",
    "                        self.action = np.argmax(self.logits, axis = 1).squeeze() #######\n",
    "                        #print(\"ACTION\", self.action)\n",
    "                        #action = np.random.choice(action_n)\n",
    "                        #stochastic_action_probabilities = tf.nn.softmax(logits)\n",
    "                        #action = np.random.choice(action_n, p=stochastic_action_probabilities.numpy()[0])\n",
    "                    else: # Greedy\n",
    "                        self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                        #print(\"self.stochastic_action_probabilities.numpy()[0]\",self.stochastic_action_probabilities.numpy()[0])\n",
    "                        \n",
    "                        self.action = np.array([])\n",
    "                        #print(self.action, self.action_n, self.stochastic_action_probabilities.numpy())\n",
    "                        for i in range(self.num_envs_per_worker):\n",
    "                            #self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[0][i])))\n",
    "                            self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[i])))      #########                      \n",
    "                        #print(i, \"self.action 1\", self.action)\n",
    "                        #print(1, self.stochastic_action_probabilities, self.action)\n",
    "                        #action = np.argmax(logits) # Total Collapse\n",
    "                else:\n",
    "                    #self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                    #self.action = np.random.choice(self.action_n, p=self.stochastic_action_probabilities.numpy()[0])\n",
    "                    \n",
    "                    self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                    #print(\"self.stochastic_action_probabilities.numpy()[0]\",self.stochastic_action_probabilities.numpy()[0])\n",
    "\n",
    "                    self.action = np.array([])\n",
    "                    for i in range(self.num_envs_per_worker):\n",
    "                        #self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[0][i])))\n",
    "                        self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[i]))) ########\n",
    "\n",
    "                    #print(2, self.stochastic_action_probabilities, self.action)                        \n",
    "                self.action = self.action.astype(int)\n",
    "            \n",
    "                #print(i, \"self.action 2\", self.action)\n",
    "                #for i in self.action:\n",
    "                #    print(type(i))\n",
    "                \n",
    "                #print(\"self.action\", self.action)\n",
    "                self.next_state, self.reward, self.done, self.info = self.env.step(self.action)\n",
    "\n",
    "                #print(\"NEXT STATE\", self.next_state)\n",
    "                #print(\"STEP 1\", self.next_state, self.reward, self.done, self.info)\n",
    "                \n",
    "                # DOES NOT WORK WITH VECTORIZED ENVIRONMENTS\n",
    "                #self.maybe_end_state = self.env.monitor.flush(force=True)\n",
    "                #print(\"maybe next state\", np.round(self.maybe_end_state,4), np.round(self.next_state,4))\n",
    "                \n",
    "                #print(\"current_state/next_state\", np.round(self.current_state[:,0],4), np.round(self.next_state[:,0],4))\n",
    "                \n",
    "                #print(\"AFTER STEP\", self.done)\n",
    "\n",
    "                #NEXT STEP (3, 4) (3,) (3,) 3\n",
    "                \n",
    "                #print(\"NEXT STEP\", type(self.next_state), type(self.reward), type(self.done), type(self.info))\n",
    "                #print(\"NEXT STEP\", self.next_state.shape, self.reward.shape, self.done.shape, len(self.info))\n",
    "                \n",
    "                #self.next_state = np.append(self.next_state, self.next_state[0] * self.next_state[0])\n",
    "                self.next_state = np.append(self.next_state,(self.next_state[:,0] * self.next_state[:,0]).reshape((-1,1)), axis = 1)\n",
    "                \n",
    "                #print(\"SQUARED\", (self.next_state[:,0] * self.next_state[:,0]).reshape((-1,1)))\n",
    "                #print(\"STEP 2\", self.next_state, self.reward, self.done, self.info)                \n",
    "\n",
    "                #print(\"self.next_state\", self.next_state)\n",
    "                # Add desired-behaviour incentive to the reward function\n",
    "                self.R_pos = 1*(1-np.abs(self.next_state[:, 0])/2.4) # 2.4 max value ### !!! in documentation it says 4.8 but failes beyound 2.4\n",
    "                self.R_ang = 1*(1-np.abs(self.next_state[:, 2])/0.20943951023931953) ### !!! in documentation it says 0.418 max value\n",
    "\n",
    "                #print(\"SHAPES BEFORE\", self.reward.shape, self.R_pos.shape, self.R_ang.shape)\n",
    "                #print(\"REWARD BEFORE\", self.reward, self.R_pos, self.R_ang)\n",
    "                self.reward = self.reward + self.R_pos + self.R_ang\n",
    "                \n",
    "                #print(\"self.reward AFTER\", self.reward)\n",
    "                \n",
    "                # Custom Fail Reward to speed up Learning of conseqences of being in advarse position\n",
    "                #print(\"self.done\", self.done)\n",
    "                \n",
    "                rows = np.where(self.done == True)\n",
    "                \n",
    "                #print(\"BEFORE self.reward NEGATIVE\", self.reward, rows)\n",
    "                self.reward[rows] = self.reward_negative # ST Original -1\n",
    "                #self.reward[rows-1] = self.reward_negative # ST Original -1\n",
    "                #print(\"AFTER self.reward NEGATIVE\", self.reward, rows)                \n",
    "                #if self.done == True: \n",
    "                #    self.reward = self.reward_negative # ST Original -1\n",
    "                        \n",
    "                #current_observation = np.append(current_state,(reward, done, action))\n",
    "                \n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 0 (3, 5) (16,)\n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 1 (31,) (3,) (3,) (3,)\n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 2 (1, 13, 3) (40,)\n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 0\", self.current_state.shape, self.next_state.shape)\n",
    "                \n",
    "                #print(\"BEFORE STACKING\", self.current_state, self.next_state)\n",
    "                #self.current_observation = np.append(self.current_state, self.next_state)\n",
    "                self.current_observation = np.concatenate([self.current_state, self.next_state], axis = 1)\n",
    "                #print(\"AFTER STACKING\", self.current_observation)\n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 1\", self.current_observation.shape, self.reward.shape, self.done.shape, self.action.shape)\n",
    "                #print(\"BEFORE STACKING 1\", self.current_observation, self.reward, self.done, self.action)\n",
    "                \n",
    "                #self.current_observation = np.append(self.current_observation,(self.reward, self.done, self.action))\n",
    "                self.current_observation = np.concatenate([self.current_observation, self.reward.reshape([-1,1]), self.done.reshape([-1,1]), self.action.reshape([-1,1])], axis = 1)\n",
    "                \n",
    "                #print(\"AFTER STACKING 1\", self.current_observation)\n",
    "                \n",
    "                #print(\"BEFORE RESHAPE\", self.current_observation)\n",
    "                #print(\"AFTER RESHAPE\", self.current_observation.reshape([1,13,self.num_envs_per_worker]))\n",
    "                #print(\"EXPAND DIM\", np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0))\n",
    "                \n",
    "                #print(\"BEFORE RESHAPE\", self.current_observation.shape)\n",
    "                #print(\"AFTER RESHAPE\", self.current_observation.reshape([1,13,self.num_envs_per_worker]).shape)\n",
    "                #print(\"EXPAND DIM\", np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0).shape)                \n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 2\", self.observations.shape, self.current_observation.shape, self.current_observation[0])\n",
    "                #self.observations = np.vstack((self.observations, self.current_observation.reshape([1,13,self.num_envs_per_worker])))\n",
    "                \n",
    "                #print(\"BEFORE SWAPPING\", self.current_observation)\n",
    "                self.observations = np.vstack((self.observations, np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0)))\n",
    "                #print(\"AFTER SWAPPING\", self.observations)\n",
    "                \n",
    "                if (self.observations.shape[0] > 1) and ((self.observations.shape[0] % self.experience_max_batch_size) == 0):\n",
    "                    self.observations = self.observations[-self.experience_max_batch_size:,:,:]\n",
    "                #print(\"SHAPES AFTER STACKING \", self.observations.shape, self.observations[:,:,0])\n",
    "\n",
    "                self.current_state = self.next_state\n",
    "                self.internal_step_counter += 1\n",
    "                  \n",
    "                if self.internal_step_counter == 1:\n",
    "                    self.observations = self.observations[1:,:,:]\n",
    "                    #print(\"self.observations\",self.observations.shape, self.observations)\n",
    "                \n",
    "                #print(\"self.done END\", self.done)\n",
    "                \n",
    "                if self.done.any() == True:\n",
    "                    self.done_ind = self.done * 1\n",
    "                    #print(\"self.done_ind\", self.done_ind, self.done_ind.sum(), self.done_done_all)\n",
    "                    self.done_ind_new = np.where((self.done_ind - self.done_done_all) == 1)\n",
    "                    #print(\"self.done_ind_new\", self.done_ind_new, self.done_ind_new[0].shape)\n",
    "                    \n",
    "                    if self.done_ind_new[0].shape[0] > 0:\n",
    "                        for exp in self.done_ind_new[0]:\n",
    "                            \n",
    "                            #print(\"exp\", exp)\n",
    "                            self.observations_current = self.observations[:,:,exp]\n",
    "                            self.terminal_state = self.info[exp][\"terminal_observation\"]\n",
    "                            #print(\"1 CURRENT STATE [-1]\", self.observations_current[-1], \"TERMINAL STATE\", self.terminal_state)\n",
    "                            self.terminal_state = np.append(self.terminal_state, self.terminal_state[0] * self.terminal_state[0])\n",
    "                            #print(\"self.terminal_state SHAPE\", self.terminal_state.shape)\n",
    "                            self.observations_current[-1,5:10] = self.terminal_state\n",
    "                            \n",
    "                            #self.observations_current = self.observations_current[1:,:]\n",
    "                            \n",
    "                            #print(\"CURRENT STATE\", self.observations_current, self.done)\n",
    "                            #print(\"self.info\", self.info)\n",
    "                            #print(\"CURRENT STATE [-1]\", self.observations_current[-1])\n",
    "                            #print(\"2 CURRENT STATE [-1]\", self.observations_current[-1], \"TERMINAL STATE\", self.terminal_state)\n",
    "\n",
    "                            self.collect_obs_length.append(self.observations_current.shape[0]) ### WRONG as now max length is self.experience_max_batch_size\n",
    "                            #print(\"self.collect_obs_length LEN()\", len(self.collect_obs_length), \"self.observations_current.shape[0]\", self.observations_current.shape[0])\n",
    "\n",
    "                            self.observations_current = self.observations_current[-np.minimum(self.observations_current.shape[0], self.experience_max_batch_size):]\n",
    "                            #print(\"self.observations_current\", self.observations_current)\n",
    "\n",
    "                            self.exp_len = self.observations_current.shape[0]\n",
    "                            #print(\"self.exp_len\", self.exp_len)\n",
    "                            self.exp_indices = np.array(range(self.exp_len)) + 1\n",
    "                            #print()\n",
    "                            self.rewards = np.flip(self.observations_current[:,(self.state_n + self.state_n_adj) * 2 ])\n",
    "                            \n",
    "                            # !!!!!!!! POTENTIAL NEGATIVE REWARD UPDATE !!!!!!!!\n",
    "                            #self.reward[1] = self.reward_negative\n",
    "                            \n",
    "                            #print(\"selfrewards\",self.rewards)\n",
    "                            self.discounted_rewards = np.empty(self.exp_len)\n",
    "                            #print(\"self.discounted_rewards\", self.discounted_rewards)\n",
    "                            self.reward_sum = 0\n",
    "\n",
    "                            if self.observations_current[-1,-2] == 0:\n",
    "                                # IN CASE THE EPISODE HAS NTO TERMINATED\n",
    "                                self.observations_current[-1,-2] = 2                        \n",
    "                                self.gamma = np.full(self.exp_len, 0.99)\n",
    "                                #print(1)\n",
    "                            else:\n",
    "                                # IN CASE THE EPISODE HAS TERMINATED\n",
    "                                #print(2)\n",
    "                                #print(\"exp_indices\", exp_indices)\n",
    "                                self.gamma = np.clip(0.0379 * np.log(self.exp_indices-1) + 0.7983, 0.5, 0.99)\n",
    "                            #print(\"END SHAPE\", self.observations.shape)    \n",
    "                            if self.observations_current[-1,-2] == 1:\n",
    "                                self.gamma[0] = 1\n",
    "                                #print(3)\n",
    "\n",
    "                            for step in range(self.exp_len):\n",
    "                                self.reward_sum = self.rewards[step] + self.gamma[step] * self.reward_sum\n",
    "                                self.discounted_rewards[step] = self.reward_sum    \n",
    "\n",
    "                            self.discounted_rewards = np.flip(self.discounted_rewards)\n",
    "                            \n",
    "                            #print(\"BEFORE self.observations_current\", np.round(np.float32(self.observations_current),4))\n",
    "                            #print(\"BEFORE self.discounted_rewards\", np.round(np.float32(self.discounted_rewards),4))\n",
    "\n",
    "                            self.observations_current = np.hstack((self.observations_current, np.expand_dims(self.discounted_rewards, axis = 1)))\n",
    "                            #print(\"AFTER self.observations_current\", np.round(np.float32(self.observations_current),4))                            \n",
    "                            self.observations_current = np.hstack((self.observations_current, np.zeros((self.observations_current.shape[0], self.pmb_cols))))\n",
    "\n",
    "                            #print(self.worker, ray.get(learner.get_episode_counter.remote()), \"observations\", self.observations_current.shape)\n",
    "                            \n",
    "                            #print(self.observations_current.shape, np.round(self.observations_current,4))\n",
    "                            \n",
    "                            # !!!!!!!! POTENTIAL NEGATIVE REWARD UPDATE !!!!!!!!\n",
    "                            #self.collect_obs.append(self.observations_current[:-1,:])\n",
    "                            \n",
    "                            \n",
    "                            self.collect_obs.append(self.observations_current)\n",
    "                            \n",
    "                            #print(\"EXECUTOR - self.collect_obs LEN\", len(self.collect_obs), \"self.observations_current.shape\", self.observations_current.shape)\n",
    "                            \n",
    "                            self.internal_step_counter_all += self.internal_step_counter\n",
    "                            \n",
    "\n",
    "                            #print(\"BEFORE\", self.done_done_all)\n",
    "                            self.done_done_all = np.maximum(self.done_done_all, self.done_ind)\n",
    "                            \n",
    "                            self.internal_step_counter_joint.append(self.internal_step_counter)\n",
    "\n",
    "                            #print(\"AFTER\", self.done_done_all, self.done_done_all.sum(), self.num_envs_per_worker)\n",
    "                            \n",
    "                            ray.get(learner.increase_episode_counter.remote())                                                     \n",
    "\n",
    "                        if self.done_done_all.sum() == self.num_envs_per_worker:\n",
    "\n",
    "                            #print(\"1\")\n",
    "                            self.done_indeed = 1\n",
    "                            ray.get(learner.increase_executor_counter.remote())                                \n",
    "\n",
    "                            #print(\"self.collect_obs\", self.collect_obs)\n",
    "\n",
    "                            #print(\"2\")\n",
    "\n",
    "                            # Update Counters to Track Progress\n",
    "\n",
    "\n",
    "                            #self.internal_step_counter_all += self.internal_step_counter\n",
    "\n",
    "\n",
    "                            #print(\"3\")\n",
    "                            if self.internal_step_counter_all < self.experience_batch_size:\n",
    "                                pass\n",
    "                            else:\n",
    "                                self.internal_step_counter_all = 0  \n",
    "                                ray.get(learner.increase_executor_counter.remote())\n",
    "\n",
    "                            print(\"Ending Executor:\", self.worker, \"Episode\", ray.get(learner.get_episode_counter.remote()), \"Initial State\", self.pos_start, \"Steps:\", self.internal_step_counter_joint)\n",
    "\n",
    "                            #print(\"4\")\n",
    "                            if self.internal_step_counter >= ray.get(learner.get_internal_step_counter_best.remote()):\n",
    "                                learner.set_internal_step_counter_best.remote(self.internal_step_counter)\n",
    "\n",
    "                                print(\"############################## BEST EPISODE LENGTH:\", self.internal_step_counter, \"Executor:\", self.worker)       \n",
    "\n",
    "                            #print(\"5\")\n",
    "                            if ray.get(learner.get_internal_step_counter_best.remote()) >= self.internal_step_counter_limit:\n",
    "\n",
    "                                self.episod_counter_target = ray.get(learner.get_episode_counter.remote())\n",
    "\n",
    "                                print(\"\\nREACHED GOAL of\", self.internal_step_counter_limit,  \"Steps in\", self.episod_counter_target, \"episodes; Learning Iterations (Not Available); in\",time.time()-self.time_start, \"seconds \\n\")  \n",
    "\n",
    "                            #ray.get(memorizer.collect.remote(learner, self.collect_obs))\n",
    "                            ray.get(memorizer.collect.remote(learner, self.collect_obs))                             \n",
    "                            ray.get(memorizer.collect_length.remote(self.collect_obs_length))\n",
    "                            \n",
    "                            self.collect_obs = []\n",
    "                            self.collect_obs_length = []                                \n",
    "\n",
    "                            #print(\"6\")\n",
    "                            while ray.get(learner.get_executor_counter.remote()) >= self.num_envs_per_worker:\n",
    "                                #print(\"WAITING FOR A NEW MODEL\")\n",
    "                                pass\n",
    "\n",
    "                            reload_model_weights = 1\n",
    "                        \n",
    "            #self.observations = np.empty((1,(self.state_n + self.state_n_adj) * 2 + 3, self.num_envs_per_worker))\n",
    "            #self.done_indeed = 0\n",
    "                \n",
    "\n",
    "@ray.remote\n",
    "def train(i, memorizer, learner, args):\n",
    "    #@ray.remote\n",
    "    #def train(i, learner):    \n",
    "    #executor = Executor.remote(memorizer, learner)\n",
    "    executor = Executor.remote(i, args)\n",
    "    ray.get(executor.experience_generator.remote(i, learner, memorizer))\n",
    "    #executor.experience_generator.remote(i, learner, memorizer)\n",
    "    \n",
    "    return \"DONE! \" + str(i)\n",
    "    \n",
    "learner = Learner.remote()\n",
    "memorizer = Memorizer.remote(learner)\n",
    "#executor = Executor.remote(memorizer, learner)    \n",
    "\n",
    "time_start = time.time()\n",
    "results = ray.get([train.remote(i, memorizer, learner, args) for i in range(int(args[\"executors_n\"]))])\n",
    "#results = ray.get([train.remote(i, learner) for i in range(int(cores))])\n",
    "print(\"RESULTS\", results, time.time()-time_start)\n",
    "    \n",
    "#print(1, ray.get(learner.get_episode_counter.remote()))\n",
    "#ray.get(learner.reset_episode_counter.remote())\n",
    "#print(2, ray.get(learner.get_episode_counter.remote()))\n",
    "\n",
    "# Do function from which to run it and initialize executors within each instance of the function.\n",
    "# within executor run memorizer collect function that will collect experience to the moemory buffer and check if enough experience has been collected and if so it will run the memorization proces - possibly memorization process can be coded in a separate fucntion\n",
    "\n",
    "\n",
    "print(ray.get(learner.get_executor_counter.remote()))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e25051",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
