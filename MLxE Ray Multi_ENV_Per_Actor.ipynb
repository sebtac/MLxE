{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df910e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### DRAFT ##################\n",
    "Implementation of GYM's Multi_ENV_Per_Actor parameter with MLxE architecture.\n",
    "\n",
    "- Implementation with RAY works\n",
    "- To finalize the work VECTORIZED GYM ENVIRONMENTS needs custom RESET method to allow proper RESET at the end of the episode\n",
    "\n",
    "See StackOverflow issue:\n",
    "    https://stackoverflow.com/questions/75551863/vectorized-gym-environments-how-to-block-automatic-environment-reset-on-done-t\n",
    "        \n",
    "Possibel ways to fix it:\n",
    "    - change the guts of the GYM package.\n",
    "    - checkout gym3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3509a9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 12:57:37,113\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(Memorizer pid=4485)\u001b[0m 2023-02-26 12:57:40.615139: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "\u001b[2m\u001b[36m(Memorizer pid=4485)\u001b[0m 2023-02-26 12:57:40.615280: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m 2023-02-26 12:57:40.613277: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m 2023-02-26 12:57:40.613408: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Memorizer pid=4485)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:47: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(train pid=4471)\u001b[0m   min_version = LooseVersion(INCLUSIVE_MIN_TF_VERSION)\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   import imp\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m 2023-02-26 12:57:43.603720: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m 2023-02-26 12:57:43.603834: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:47: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m   min_version = LooseVersion(INCLUSIVE_MIN_TF_VERSION)\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m /var/folders/h4/d_xxpqw56t95qlbw1w07l3340000gn/T/ipykernel_4424/3356709775.py:947: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 10 Initial State [4 0 1 4 3 3 3 0 4 4] Steps: [8, 10, 11, 14, 16, 17, 17, 18, 26, 58]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 58 Executor: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:57:46,056 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592241152; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 64 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 20 Initial State [0 2 2 1 2 1 3 2 2 2] Steps: [8, 10, 11, 13, 13, 13, 13, 16, 16, 39]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 128 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 30 Initial State [0 3 2 3 1 0 2 3 4 2] Steps: [10, 10, 12, 12, 16, 17, 19, 19, 20, 28]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 192 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 40 Initial State [2 4 2 1 2 2 3 2 2 1] Steps: [11, 13, 13, 14, 16, 16, 18, 22, 22, 45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:57:56,130 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592237056; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 256 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 50 Initial State [2 2 2 2 2 1 2 3 2 2] Steps: [10, 10, 13, 19, 22, 23, 24, 25, 26, 38]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 320 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 60 Initial State [2 2 1 2 3 2 2 2 2 4] Steps: [8, 10, 10, 10, 13, 13, 15, 17, 25, 28]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 384 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 70 Initial State [2 2 1 4 2 2 2 4 2 2] Steps: [8, 12, 12, 12, 14, 14, 18, 23, 25, 26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:06,213 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592237056; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 448 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 80 Initial State [3 2 2 2 2 4 1 2 2 2] Steps: [8, 8, 9, 9, 10, 11, 15, 16, 18, 27]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 512 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 90 Initial State [2 2 1 2 2 2 2 2 2 1] Steps: [11, 15, 15, 16, 18, 21, 21, 25, 25, 26]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 576 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 100 Initial State [2 2 2 1 2 2 2 2 2 2] Steps: [9, 10, 11, 12, 12, 13, 18, 21, 27, 27]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 640 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 110 Initial State [0 4 0 2 4 3 1 2 2 2] Steps: [8, 8, 8, 9, 10, 10, 10, 10, 11, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:16,283 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592089600; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 704 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 120 Initial State [0 2 2 2 2 2 2 1 2 3] Steps: [9, 9, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 768 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 130 Initial State [2 2 2 2 0 2 3 3 2 2] Steps: [8, 9, 9, 9, 10, 10, 10, 10, 10, 11]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 832 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 140 Initial State [2 1 0 2 3 2 2 2 2 2] Steps: [8, 8, 8, 8, 9, 9, 9, 10, 10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:26,356 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592089600; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 896 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 150 Initial State [2 3 1 2 2 2 2 2 2 1] Steps: [8, 8, 9, 9, 9, 9, 10, 10, 10, 10]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 960 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 160 Initial State [2 2 3 0 2 2 2 3 2 4] Steps: [8, 9, 9, 9, 9, 9, 9, 10, 10, 10]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1024 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 170 Initial State [4 2 2 2 4 0 2 2 2 1] Steps: [8, 8, 9, 9, 9, 9, 9, 10, 10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:36,432 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592089600; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1088 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 180 Initial State [0 2 2 2 4 2 2 2 2 2] Steps: [23, 24, 26, 28, 29, 32, 36, 37, 39, 41]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1152 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 190 Initial State [4 0 0 2 2 2 2 2 2 2] Steps: [103, 107, 108, 110, 110, 114, 142, 175, 177, 238]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 238 Executor: 0\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1216 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 200 Initial State [2 4 0 2 2 0 2 2 4 1] Steps: [35, 37, 39, 40, 42, 48, 51, 52, 76, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:46,503 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592028160; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1280 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 210 Initial State [2 2 3 2 2 2 2 0 2 0] Steps: [28, 30, 34, 39, 46, 48, 48, 72, 74, 83]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1344 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 220 Initial State [3 2 2 2 2 2 2 2 2 2] Steps: [48, 55, 59, 72, 72, 76, 94, 102, 105, 146]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1408 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 230 Initial State [1 1 3 2 1 0 2 1 2 4] Steps: [36, 40, 40, 42, 44, 45, 46, 58, 64, 67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:56,582 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592032256; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1472 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 240 Initial State [2 2 3 0 2 2 1 2 2 4] Steps: [49, 54, 58, 58, 63, 64, 64, 66, 73, 82]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1536 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 250 Initial State [3 3 0 2 2 2 2 2 2 2] Steps: [76, 78, 85, 96, 105, 106, 107, 134, 135, 165]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1600 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 260 Initial State [2 1 2 2 4 4 2 2 2 2] Steps: [111, 112, 116, 135, 151, 152, 206, 235, 243, 297]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 297 Executor: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:06,661 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592032256; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1664 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 270 Initial State [2 2 2 4 2 0 2 2 2 1] Steps: [156, 170, 170, 178, 182, 191, 223, 235, 275, 286]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1728 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 280 Initial State [2 2 2 0 2 2 2 2 2 1] Steps: [162, 164, 176, 176, 194, 195, 196, 203, 213, 241]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:16,745 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59591766016; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1792 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 290 Initial State [1 2 2 2 2 2 2 2 2 2] Steps: [170, 181, 248, 254, 396, 459, 467, 485, 584, 852]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 852 Executor: 0\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1856 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 300 Initial State [2 3 2 2 4 0 2 2 2 2] Steps: [9, 9, 9, 9, 9, 10, 10, 10, 10, 11]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1920 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:26,820 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59591766016; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 310 Initial State [2 2 2 2 2 2 2 2 2 2] Steps: [30, 33, 34, 35, 38, 41, 46, 52, 63, 118]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1984 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 320 Initial State [2 2 2 1 2 2 2 2 0 2] Steps: [32, 33, 34, 34, 37, 39, 40, 65, 68, 70]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2048 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 330 Initial State [3 2 2 2 0 2 2 2 2 0] Steps: [29, 30, 31, 33, 36, 36, 38, 38, 39, 40]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2112 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:36,897 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59591811072; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 340 Initial State [0 2 2 0 2 2 2 4 1 2] Steps: [46, 52, 54, 63, 68, 70, 70, 74, 86, 92]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2176 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 350 Initial State [4 4 2 2 4 3 2 0 2 2] Steps: [50, 51, 52, 52, 59, 62, 67, 67, 72, 263]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2240 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 360 Initial State [1 2 1 2 2 3 2 2 2 2] Steps: [74, 79, 82, 88, 90, 98, 98, 101, 109, 114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:46,970 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589160960; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2304 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 370 Initial State [4 2 2 2 2 2 4 1 2 2] Steps: [67, 95, 99, 101, 102, 108, 113, 124, 147, 179]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2368 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:57,051 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589160960; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 380 Initial State [2 2 2 1 3 2 4 2 2 4] Steps: [85, 87, 95, 113, 113, 120, 121, 153, 399, 2155]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 2155 Executor: 0\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2432 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 390 Initial State [4 2 2 4 2 2 4 2 2 2] Steps: [84, 98, 103, 117, 136, 194, 275, 392, 659, 1051]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:07,129 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589427200; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2496 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 400 Initial State [2 2 2 2 2 2 2 2 2 2] Steps: [52, 56, 68, 72, 74, 94, 135, 145, 435, 845]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2560 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 410 Initial State [2 2 2 2 2 2 4 2 4 0] Steps: [38, 39, 59, 63, 69, 130, 185, 207, 269, 274]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:17,211 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589382144; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2624 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 420 Initial State [3 0 1 2 2 2 2 2 2 2] Steps: [35, 41, 45, 45, 50, 51, 79, 156, 190, 264]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2688 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 430 Initial State [0 2 2 2 4 3 2 1 0 2] Steps: [35, 40, 45, 59, 59, 60, 72, 185, 211, 291]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2752 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 440 Initial State [2 2 2 2 0 2 2 2 2 2] Steps: [35, 47, 54, 54, 54, 65, 68, 68, 78, 79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:27,290 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589349376; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2816 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 450 Initial State [2 2 2 2 0 2 0 2 2 2] Steps: [34, 37, 41, 48, 49, 56, 60, 101, 169, 413]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2880 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 460 Initial State [2 2 2 1 2 2 2 1 2 2] Steps: [34, 39, 41, 42, 47, 49, 55, 59, 61, 170]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:37,372 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589353472; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2944 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 470 Initial State [2 0 2 2 2 0 2 2 2 2] Steps: [37, 39, 43, 46, 47, 75, 182, 295, 368, 406]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 3008 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 480 Initial State [1 0 0 2 2 2 2 2 2 2] Steps: [33, 36, 40, 51, 52, 59, 65, 69, 71, 277]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 3072 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 490 Initial State [2 0 2 4 2 2 2 2 2 2] Steps: [32, 32, 39, 44, 45, 47, 49, 49, 52, 54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:47,456 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59588194304; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 3136 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 500 Initial State [2 2 1 1 4 2 2 4 2 0] Steps: [36, 43, 51, 53, 53, 55, 69, 80, 173, 525]\n",
      "RESULTS ['DONE! 0'] 196.87191081047058\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import ray, gym, time, math\n",
    "ray.RAY_memory_monitor_refresh_ms = 0 # Should stop providing memory warinings!!! but it does not !!!! \n",
    "\n",
    "from ray.util.queue import Queue\n",
    "from ray._private.utils import get_num_cpus\n",
    "\n",
    "cores = get_num_cpus()\n",
    "collect_obs = Queue()\n",
    "collect_examples = Queue()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa \n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.20f}\".format(x)})\n",
    "np.set_printoptions(precision=4, floatmode=\"maxprec\",suppress=True, linewidth=200)\n",
    "\n",
    "args = {# SYSTEM Settings\n",
    "        \"executors_n\": 1, #get_num_cpus(), # How many Executors are running in parallel\n",
    "\n",
    "        # GYM Settings\n",
    "        \"max_episodes\": 500, # 500 How many Episodes do you want to run?\n",
    "        \"num_envs_per_worker\": 10, # how many environments to run per worker -- tested till 512!!!\n",
    "        \"env_name\": 'CartPole-v1', # the name of the GYM Environment to run\n",
    "\n",
    "        \n",
    "        # MODEL Definition\n",
    "        \"state_n\": 4, # value of env.observation_space() (*2 as current_state and next_state)\n",
    "        \"state_n_adj\": 1, # all Task Specific Adjustments (*2 as current_state and next_state)\n",
    "        \"state_n_add\": 4, # all additional step descriptions (Reward, Done, Action, Discounted Reward)\n",
    "        \"action_n\": 2, # value of env.action_space()\n",
    "        \"common_layers_n\": [128,256,128], # Number of Neurons in Common Layers of A3C Model\n",
    "        \"value_layers_n\": [64,128,64], # Number of Neurons in Value Layers of A3C Model\n",
    "        \"policy_layers_n\": [64,128,64], # Number of Neurons in Policy Layers of A3C Model\n",
    "         \n",
    "        # LEARNER Settings\n",
    "        \"batch_size\": 128, # Number of examples per model update\n",
    "        \"model_alignment_frequency\": 128, # frequency of synchronization of the Target Model with On-Line Model -- OPTIONAL\n",
    "        \"minimum_model_update_frequency\": 64, # How Many Model Updates to run in each iteration at minimum.\n",
    "        \"epsilon_decay_policy\":1, # Run EDP? 0 No, 1 Yes\n",
    "        \"epsilon\": 0.1, # Original 0.1                 # e-greedy when exploring\n",
    "        \"epsilon_decay\": 0.995, # Original 0.995         # epsilon decay r\n",
    "        \n",
    "        # LEARNING RATE Decceleration\n",
    "        \"lr_alpha\": 0.0001, # Inital LR\n",
    "        \"lr_alpha_power\": 0.998, #0.998 bast so far # Controls the pace of LR depreciation\n",
    "        \"lr_alpha_limit\": 0.000001, # Lower limit on the LR\n",
    "        \n",
    "        # EXECUTOR Settings\n",
    "        \"internal_step_counter_limit\": 50000, # 500000 limit of steps per episode \n",
    "        \"experience_batch_size\": 1024, # how many steps to save into the memory buffer from each run\n",
    "        \"experience_max_batch_size\": 256, # Maximum number of cases to save to Memory Baffer from each experience (most recent) (tested 1024-64: 256 Best)\n",
    "        \n",
    "        # advarse STATE Probability\n",
    "        \"prob_advarse_state_initial\": 0.2, # Probability of choosing advarse case when starting new episode\n",
    "        \"prob_advarse_state_type_multiplier\": 0.0, #Adjustment to the Probability to control the ratio of issue types that are promoted through advarse initialization\n",
    "        \n",
    "        # REWARD Incentives\n",
    "        \"reward_negative\": -10.0, #-10.0, # Override the GYM's default negative return\n",
    "        \n",
    "        # PRIORITY MEMORY BUFFER\n",
    "        \"pmb_cols\": 5,  #\n",
    "        \"pmb_alpha\": 0.9, # Not Implemented                          # priority parameter, alpha=[0, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "        \"pmb_beta\": 0.4, # Tested till 0.8 but best reuslts with 1.0 # importance sampling parameter, beta=[0, 0.4, 0.5, 0.6, 1]\n",
    "        \"pmb_beta_increment\": 0.001, # Originally 0.001\n",
    "        \"pmb_td_error_margin\": 0.01,                                # pi = |td_error| + margin\n",
    "        \"pmb_abs_td_error_upper\": 1,\n",
    "               \n",
    "        }\n",
    "\n",
    "@ray.remote\n",
    "class Learner:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        \n",
    "        self.collect_examples = []     \n",
    "        \n",
    "        \n",
    "        # Define BASE Model - Target\n",
    "        self.inputs_base = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "\n",
    "        self.common_network_base = Dense(self.common_layers_n[0], activation='relu',name=\"1\")(self.inputs_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[1], activation='relu',name=\"2\")(self.common_network_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[2], activation='relu',name=\"3\")(self.common_network_base)\n",
    "\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[0], activation='relu',name=\"7\")(self.common_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[1], activation='relu',name=\"8\")(self.policy_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[2], activation='relu',name=\"9\")(self.policy_network_base)\n",
    "\n",
    "        self.value_network_base = Dense(self.value_layers_n[0], activation='relu',name=\"4\")(self.common_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[1], activation='relu',name=\"5\")(self.value_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[2], activation='relu',name=\"6\")(self.value_network_base)\n",
    "\n",
    "        self.values_base = Dense(1,name=\"10\")(self.value_network_base)            \n",
    "        self.logits_base = Dense(self.action_n,name=\"11\")(self.policy_network_base)\n",
    "\n",
    "        self.model_base = Model(inputs=self.inputs_base, outputs=[self.values_base, self.logits_base])\n",
    "\n",
    "\n",
    "        # Define MAIN Model - Trainable Model\n",
    "        self.inputs_main = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "        self.common_network_main = Dense(self.common_layers_n[0], activation='relu')(self.inputs_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[1], activation='relu')(self.common_network_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[2], activation='relu')(self.common_network_main)\n",
    "\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_main)\n",
    "\n",
    "        self.value_network_main = Dense(self.value_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[1], activation='relu')(self.value_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[2], activation='relu')(self.value_network_main)\n",
    "\n",
    "        self.values_main = Dense(1)(self.value_network_main)\n",
    "        self.logits_main = Dense(self.action_n)(self.policy_network_main)\n",
    "        \n",
    "        self.model_main = Model(inputs=self.inputs_main, outputs=[self.values_main, self.logits_main])\n",
    "\n",
    "        # Define Optimizer\n",
    "        self.optimizer = tfa.optimizers.RectifiedAdam(self.lr_alpha)\n",
    "        \n",
    "        self.executor_model = self.model_main.get_weights()\n",
    "\n",
    "        #if counter_learninig % model_alignment_frequency == 0:\n",
    "        self.model_base.set_weights(self.model_main.get_weights())    \n",
    "\n",
    "        #executor_model.append(model_main.get_weights()) # the first call MUST be append to create the entry [0]\n",
    "        #print(\"Saved Model\", worker, len(executor_model))\n",
    "\n",
    "        self.memory_buffer = np.full((self.state_n+self.state_n_adj) * 2 + self.state_n_add + self.pmb_cols,0.0)\n",
    "        self.memory_buffer = []        \n",
    "\n",
    "        # GLOBAL COUNTERS -- possibly tobe moved to a separate worker\n",
    "        self.counter_learninig = 0\n",
    "        self.episode_counter = 0\n",
    "        self.executor_counter = 0        \n",
    "        self.steps_counter = 0\n",
    "        self.internal_step_counter_best = 0\n",
    "        \n",
    "    def get_executor_model(self):\n",
    "        return self.executor_model\n",
    "\n",
    "    def get_base_model_weights(self):\n",
    "        return self.model_base.get_weights()\n",
    "\n",
    "    def get_main_model_weights(self):\n",
    "        return self.model_main.get_weights()\n",
    "    \n",
    "    def increase_counter_learninig(self):\n",
    "        self.counter_learninig += 1\n",
    "\n",
    "    def increase_episode_counter(self):\n",
    "        self.episode_counter += 1\n",
    "\n",
    "    def increase_executor_counter(self):\n",
    "        self.executor_counter += 1\n",
    "        \n",
    "    def increase_steps_counter(self, value):\n",
    "        self.steps_counter += value\n",
    "\n",
    "    def set_internal_step_counter_best(self, value):\n",
    "        self.internal_step_counter_best = value\n",
    "    \n",
    "    def reset_counter_learninig(self):\n",
    "        self.counter_learninig = 0\n",
    "\n",
    "    def reset_episode_counter(self):\n",
    "        self.episode_counter = 0\n",
    "        \n",
    "    def reset_executor_counter(self):\n",
    "        self.executor_counter = 0        \n",
    "        \n",
    "    def reset_steps_counter(self):\n",
    "        self.steps_counter = 0\n",
    "        \n",
    "    def get_counter_learninig(self):\n",
    "        return self.counter_learninig\n",
    "\n",
    "    def get_episode_counter(self):\n",
    "        return self.episode_counter\n",
    "        \n",
    "    def get_executor_counter(self):\n",
    "        return self.executor_counter\n",
    "    \n",
    "    def get_steps_counter(self):\n",
    "        return self.steps_counter\n",
    "\n",
    "    def get_internal_step_counter_best(self):\n",
    "        return self.internal_step_counter_best\n",
    "\n",
    "    def train(self):\n",
    "        #print(\"LEARNER.train\")\n",
    "        #print(\"LEARNER RUN\", self.episode_counter)\n",
    "\n",
    "        # Adjust Monotonically Decreasing Learning Rate\n",
    "        self.next_lr_alpha = self.lr_alpha * np.power(self.lr_alpha_power, self.episode_counter)\n",
    "        if self.next_lr_alpha < self.lr_alpha_limit:\n",
    "            self.next_lr_alpha = self.lr_alpha_limit\n",
    "\n",
    "        self.optimizer.learning_rate = self.next_lr_alpha\n",
    "        \n",
    "        #print(\"next_lr_alpha\", self.next_lr_alpha)\n",
    "\n",
    "        # Initialize LEARNER\n",
    "        #while self.collect_examples.qsize() > 0: # and episod_counter.value < max_episodes:\n",
    "\n",
    "        #print(\"LEARNER len(self.collect_examples)\", len(self.collect_examples))\n",
    "        for i in self.memory_buffer:\n",
    "\n",
    "            self.example = i\n",
    "            #print(\"LEARNER self.example\", self.example.shape)\n",
    "            \n",
    "            #print(\"WEIGHTS BASE\", self.model_base.get_weights()[0][0][0])\n",
    "            #print(\"WEIGHTS MAIN\", self.model_main.get_weights()[0][0][0])\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                #print(\"CS\", example[:,:5])\n",
    "                self.values, self.logits = self.model_base(tf.convert_to_tensor(self.example[:,:5], dtype=tf.float32))\n",
    "                #print(\"Disc Reward\", example[:,-(pmb_cols+1)])\n",
    "                self.advantage = tf.convert_to_tensor(np.expand_dims(self.example[:,-(self.pmb_cols+1)],axis=1), dtype=tf.float32) - self.values\n",
    "                self.value_loss = self.advantage ** 2 # this is a term to be minimized in trainig \n",
    "                self.policy = tf.nn.softmax(self.logits)\n",
    "                self.entropy = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(labels=self.policy, logits=self.logits), [-1,1])\n",
    "                #print(\"Action\", example[:,-(pmb_cols+2)])\n",
    "                self.policy_loss = tf.reshape(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=list(self.example[:,-(self.pmb_cols+2)].astype(int)), logits=self.logits), [-1,1])            \n",
    "                self.policy_loss *= tf.stop_gradient(self.advantage) # advantage will be exluded from computation of the gradient; thsi allows to treat the values as constants\n",
    "                self.policy_loss -= 0.01 * self.entropy # entropy adjustment for better exploration \n",
    "                self.total_loss = tf.reduce_mean((0.5 * self.value_loss + self.policy_loss))\n",
    "\n",
    "\n",
    "            self.grads = tape.gradient(self.total_loss, self.model_base.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(self.grads, self.model_main.trainable_weights))\n",
    "            \n",
    "            self.counter_learninig += 1\n",
    "\n",
    "            self.model_base.set_weights(self.model_main.get_weights()) ### the THREADED IMPLEMENTATION IS SYNCHRONIZED AT EACH STEP!!!\n",
    "\n",
    "        self.executor_model = self.model_main.get_weights()\n",
    "        \n",
    "        #print(\"LEARNER T BASE\", self.model_base.get_weights()[0][0][:5])\n",
    "        #print(\"LEARNER T MAIN\", self.model_main.get_weights()[0][0][:5])\n",
    "        \n",
    "        #print(type(self.executor_model))\n",
    "        #print(\"LEARNER T EXECUTOR\", self.executor_model[0][0][:5])        \n",
    "\n",
    "        #if counter_learninig % model_alignment_frequency == 0:\n",
    "        self.model_base.set_weights(self.model_main.get_weights())\n",
    "\n",
    "        print(\"LEARINING ITERATION:\", self.counter_learninig,\"\\n\")\n",
    "        \n",
    "        #self.steps_counter = 0\n",
    "        self.executor_counter = 0\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def update_memory_buffer(self, experience):\n",
    "        \n",
    "        #self.memory_buffer.append(experience)\n",
    "        self.memory_buffer = experience\n",
    "        #print(\"LEARNER UPDATE MB\", len(self.memory_buffer))\n",
    "        \n",
    "@ray.remote\n",
    "class Memorizer:\n",
    "    def __init__(self, learner):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        \n",
    "        self.memory_buffer = np.full((self.state_n + self.state_n_adj) * 2 + self.state_n_add + self.pmb_cols,0.0)\n",
    "        self.memory_buffer[-1] = 99 \n",
    "        self.collect_obs = []\n",
    "        self.worker = \"Memorizer\"\n",
    "        \n",
    "        # Define BASE Model - Target\n",
    "        self.inputs_base = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "\n",
    "        self.common_network_base = Dense(self.common_layers_n[0], activation='relu',name=\"1\")(self.inputs_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[1], activation='relu',name=\"2\")(self.common_network_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[2], activation='relu',name=\"3\")(self.common_network_base)\n",
    "\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[0], activation='relu',name=\"7\")(self.common_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[1], activation='relu',name=\"8\")(self.policy_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[2], activation='relu',name=\"9\")(self.policy_network_base)\n",
    "\n",
    "        self.value_network_base = Dense(self.value_layers_n[0], activation='relu',name=\"4\")(self.common_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[1], activation='relu',name=\"5\")(self.value_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[2], activation='relu',name=\"6\")(self.value_network_base)\n",
    "\n",
    "        self.values_base = Dense(1,name=\"10\")(self.value_network_base)            \n",
    "        self.logits_base = Dense(self.action_n,name=\"11\")(self.policy_network_base)\n",
    "\n",
    "        self.model_base = Model(inputs=self.inputs_base, outputs=[self.values_base, self.logits_base])   \n",
    "        self.base_model_weights = ray.get(learner.get_base_model_weights.remote())\n",
    "        self.model_base.set_weights(self.base_model_weights)\n",
    "        \n",
    "        # Define MAIN Model - Trainable Model\n",
    "        self.inputs_main = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "        self.common_network_main = Dense(self.common_layers_n[0], activation='relu')(self.inputs_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[1], activation='relu')(self.common_network_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[2], activation='relu')(self.common_network_main)\n",
    "\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_main)\n",
    "\n",
    "        self.value_network_main = Dense(self.value_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[1], activation='relu')(self.value_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[2], activation='relu')(self.value_network_main)\n",
    "\n",
    "        self.values_main = Dense(1)(self.value_network_main)\n",
    "        self.logits_main = Dense(self.action_n)(self.policy_network_main)\n",
    "        \n",
    "        self.model_main = Model(inputs=self.inputs_main, outputs=[self.values_main, self.logits_main])\n",
    "        self.main_model_weights = ray.get(learner.get_main_model_weights.remote())\n",
    "        self.model_main.set_weights(self.main_model_weights) \n",
    "        \n",
    "        #self.experience_length_all = np.array([]).reshape(0, self.executors_n)\n",
    "        self.experience_length_all = []\n",
    "        self.experience_length = []\n",
    "        \n",
    "    \n",
    "    def get_experience_length_all(self):\n",
    "        return self.experience_length_all[1:]\n",
    "    \n",
    "    def get_collected_obs(self):\n",
    "        return self.collect_obs\n",
    "        \n",
    "    def collect(self, learner, experience):\n",
    "        #print(\"MEMORIZER.collect\")\n",
    "        \n",
    "        #print(\"MEMORIZER.collect - len(experience)\", len(experience), \"experience\")\n",
    "        self.collect_obs.append(experience)\n",
    "        \n",
    "        #print(\"MEMORIZER - EXECUTORS\", ray.get(learner.get_executor_counter.remote()))\n",
    "        \n",
    "        if ray.get(learner.get_executor_counter.remote()) < self.executors_n:\n",
    "            #print(\"WAITING IN MEMORIZER 1\", ray.get(learner.get_executor_counter.remote()), ray.get(learner.get_steps_counter.remote()), ray.get(learner.get_episode_counter.remote()))\n",
    "            pass\n",
    "        else:\n",
    "            #print(\"WAITING IN MEMORIZER 2\")\n",
    "            #print(\"MEMORY UPDATE\", self.collect_obs)\n",
    "            self.memory_update(learner, self.collect_obs)\n",
    "            #ray.get(learner.update_memory_buffer.remote(self.memory_buffer))\n",
    "            \n",
    "    def collect_length(self, experience_length):\n",
    "        self.experience_length.append(experience_length)\n",
    "        #print(\"MEMORIZER.collect_length\")\n",
    "            \n",
    "    #def memory_update(self, learner, collect_obs, collect_examples):\n",
    "    def memory_update(self, learner, collect_obs):\n",
    "        #print(\"MEMORIZER.memory_update\")\n",
    "        \n",
    "        while ray.get(learner.get_executor_counter.remote()) < self.executors_n:\n",
    "            pass\n",
    "        \n",
    "        self.base_model_weights = ray.get(learner.get_base_model_weights.remote())\n",
    "        self.model_base.set_weights(self.base_model_weights)\n",
    "        \n",
    "        self.main_model_weights = ray.get(learner.get_main_model_weights.remote())\n",
    "        self.model_main.set_weights(self.main_model_weights)\n",
    "        \n",
    "        #print(\"MEMORIZER MU BASE\", self.model_base.get_weights()[0][0][:5])\n",
    "        #print(\"MEMORIZER MU MAIN\", self.model_main.get_weights()[0][0][:5])\n",
    "        \n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        #self.experience_length = np.zeros((1,self.executors_n))\n",
    "        #self.experience_length = []\n",
    "        \n",
    "        #print(\"BEFORE\", len(self.experience_length))\n",
    "        \n",
    "        #print(\"collect_obs\", len(collect_obs[0]), \"collect_obs\")\n",
    "        #print(\"BEFORE MEMORY BUFFER\", self.memory_buffer.shape, self.memory_buffer)\n",
    "        for i in collect_obs[0]:\n",
    "            self.mem_counter += 1\n",
    "            self.exp_temp = i\n",
    "            \n",
    "            #print(\"self.exp_temp\", self.exp_temp.shape, \"self.exp_temp\", \"self.exp_temp\")\n",
    "            \n",
    "            #self.experience_length[0,self.mem_counter] = self.exp_temp.shape[0]\n",
    "            #self.experience_length.append(self.exp_temp.shape[0])\n",
    "\n",
    "            #print(self.worker, \"self.exp_temp\", type(self.exp_temp), self.exp_temp)\n",
    "\n",
    "            #print(self.worker, mem_counter, \"memory_buffer B_STACK\", self.memory_buffer.shape)\n",
    "            #print(self.worker, mem_counter, \"exp_temp B_STACK\", self.exp_temp.shape)\n",
    "\n",
    "            self.memory_buffer = np.vstack((self.memory_buffer, self.exp_temp))\n",
    "            #print(self.worker, self.mem_counter, \"memory_buffer A_STACK\", self.memory_buffer.shape)\n",
    "            #print(self.worker, mem_counter, \"MIN INPUTS\", self.memory_buffer.shape[0], self.experience_batch_size * self.executors_n) \n",
    "            \n",
    "            self.memory_buffer = self.memory_buffer[-np.minimum(self.memory_buffer.shape[0], self.experience_batch_size * self.executors_n):,:]\n",
    "\n",
    "            #print(self.worker, self.mem_counter, \"memory_buffer A_MIN\", self.memory_buffer.shape)\n",
    "        \n",
    "        #self.experience_length_all = np.vstack([self.experience_length_all, self.experience_length])\n",
    "        \n",
    "        if self.memory_buffer[0,-1] == 99:\n",
    "            self.memory_buffer = self.memory_buffer[1:,:]\n",
    "            \n",
    "        #print(\"AFTER MEMORY BUFFER\", self.memory_buffer.shape, self.memory_buffer)\n",
    "        #print(\"AFTER\", len(self.experience_length))\n",
    "        \n",
    "        self.experience_length_all.append(self.experience_length)\n",
    "        \n",
    "        # PRIORITY MEMORY BUFFER\n",
    "\n",
    "        # Inverse Discounted Reward Probability\n",
    "        self.dr_min = np.min(self.memory_buffer[:,-(self.pmb_cols+1)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = self.memory_buffer[:,-(self.pmb_cols+1)] - self.dr_min\n",
    "        self.dr_max = np.max(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = 1 - self.memory_buffer[:,-(self.pmb_cols)] / self.dr_max + 0.01\n",
    "        self.dr_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = self.memory_buffer[:,-(self.pmb_cols)] / self.dr_sum\n",
    "\n",
    "        # Inverse \"Age\" Probability\n",
    "        self.memory_buffer[:,-(self.pmb_cols-1)] += 1\n",
    "        self.age_max = np.max(self.memory_buffer[:,-(self.pmb_cols-1)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-2)] = self.age_max - self.memory_buffer[:,-(self.pmb_cols-1)] + 1.0\n",
    "        self.age_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols-2)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-2)] = self.memory_buffer[:,-(self.pmb_cols-2)] / self.age_sum\n",
    "\n",
    "        # Proportional TD Error Probability                \n",
    "        self.target_q, self.target_logits = self.model_base(tf.convert_to_tensor(self.memory_buffer[:,5:10], dtype=tf.float32))\n",
    "        self.td_target = np.expand_dims(self.memory_buffer[:,10],axis = -1) + 0.9 * self.target_q * np.expand_dims((1 - self.memory_buffer[:,11]),axis = -1)\n",
    "        self.predict_q, self.predict_logits = self.model_main(tf.convert_to_tensor(self.memory_buffer[:,:5], dtype=tf.float32))\n",
    "        self.abs_td_error = np.abs(self.td_target - self.predict_q) + self.pmb_td_error_margin\n",
    "        self.clipped_td_error = np.where(self.abs_td_error < self.pmb_abs_td_error_upper, self.abs_td_error, self.pmb_abs_td_error_upper)                \n",
    "        self.memory_buffer[:,-(self.pmb_cols-3)] = self.clipped_td_error[:,0]\n",
    "        self.td_error_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols-3)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-3)] = self.memory_buffer[:,-(self.pmb_cols-3)] / self.td_error_sum\n",
    "\n",
    "        self.memory_buffer[:,-1] = np.average(self.memory_buffer[:,[-(self.pmb_cols-2),-(self.pmb_cols-3)]], axis = 1 ) # Best! 280-310\n",
    "        self.pmb_beta = min(1., self.pmb_beta + self.pmb_beta_increment * self.executors_n)\n",
    "        self.memory_buffer[:,-1] = np.power(self.memory_buffer[:,-1], self.pmb_beta)\n",
    "        self.total_error_sum = np.sum(self.memory_buffer[:,-1])\n",
    "        self.memory_buffer[:,-1] = self.memory_buffer[:,-1] / self.total_error_sum\n",
    "\n",
    "        self.prob_sum_check1 = np.sum(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.prob_sum_check2 = np.sum(self.memory_buffer[:,-(self.pmb_cols-2)])\n",
    "        self.prob_sum_check3 = np.sum(self.memory_buffer[:,-(self.pmb_cols-3)])\n",
    "        self.prob_sum_check = np.sum(self.memory_buffer[:,-1])\n",
    "\n",
    "        self.batch_size_min = np.minimum(self.batch_size,self.memory_buffer.shape[0])\n",
    "        self.runs = self.memory_buffer.shape[0] // np.minimum(self.memory_buffer.shape[0], self.batch_size_min) + 1\n",
    "        self.runs = np.maximum(self.minimum_model_update_frequency, self.runs)\n",
    "        \n",
    "        collect_samples = []\n",
    "        \n",
    "        for i in range(self.runs):\n",
    "\n",
    "            self.sample_index = np.random.choice(self.memory_buffer.shape[0],\n",
    "                                            np.minimum(self.memory_buffer.shape[0], self.batch_size_min),\n",
    "                                            p = self.memory_buffer[:,-1],\n",
    "                                            replace=False)\n",
    "            \n",
    "            self.sample = self.memory_buffer[self.sample_index, :]\n",
    "            \n",
    "            #print(\"self.sample\", self.sample.shape, \"self.sample\")\n",
    "\n",
    "            collect_samples.append(self.sample)\n",
    "        \n",
    "        #print(\"self.sample\", self.sample)\n",
    "        \n",
    "        self.collect_obs = []\n",
    "        self.experience_length = []\n",
    "        \n",
    "        #print(\"MEMORIZER len(collect_examples)\", len(collect_examples))\n",
    "        ray.get(learner.update_memory_buffer.remote(collect_samples))\n",
    "        #print(\"MEMORIZER Before TRAIN\")\n",
    "        ray.get(learner.train.remote())\n",
    "        #print(\"MEMORIZER After TRAIN\")\n",
    "    \n",
    "@ray.remote\n",
    "class Executor:\n",
    "    #def __init__(self, memorizer, learner):\n",
    "    def __init__(self, i, args):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]/16\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        self.num_envs_per_worker = args[\"num_envs_per_worker\"]\n",
    "        \n",
    "        self.collect_examples = collect_examples\n",
    "        \n",
    "        # Establish Environment\n",
    "        #self.env = gym.make(self.env_name).unwrapped #unwrapped to access the behind the scenes elements of the environment\n",
    "        #self.env = gym.vector.make(self.env_name, num_envs=3).unwrapped #unwrapped to access the behind the scenes elements of the environment\n",
    "        \n",
    "        self.env = gym.vector.SyncVectorEnv([lambda: gym.make(self.env_name).env for _ in range(self.num_envs_per_worker)])\n",
    "        #self.env = gym.vector.SyncVectorEnv([lambda: gym.make(self.env_name) for _ in range(self.num_envs_per_worker)])\n",
    "        #self.env = gym.vector.AsyncVectorEnv([lambda: gym.make(self.env_name).env for _ in range(self.num_envs_per_worker)])\n",
    "        \n",
    "        #print(\"self.env.state\", self.env.state)\n",
    "        #print(\"self.env.state\", self.env.observations)\n",
    "        \n",
    "        #self.env = gym.vector.SyncVectorEnv([\n",
    "        #    lambda: gym.make(self.env_name),\n",
    "        #    lambda: gym.make(self.env_name),            \n",
    "        #    lambda: gym.make(self.env_name)])\n",
    "        \n",
    "        # Define A3C Model for Executors\n",
    "        #self.inputs_executor = tf.keras.Input(shape=(self.num_envs_per_worker,self.state_n + self.state_n_adj,))\n",
    "        self.inputs_executor = tf.keras.Input(shape=(self.state_n + self.state_n_adj,))\n",
    "        \n",
    "        self.common_network_executor = Dense(self.common_layers_n[0], activation='relu')(self.inputs_executor)\n",
    "        self.common_network_executor = Dense(self.common_layers_n[1], activation='relu')(self.common_network_executor)\n",
    "        self.common_network_executor = Dense(self.common_layers_n[2], activation='relu')(self.common_network_executor)\n",
    " \n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_executor)\n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_executor)\n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_executor)\n",
    "        \n",
    "        self.value_network_executor = Dense(self.value_layers_n[0], activation='relu')(self.common_network_executor)\n",
    "        self.value_network_executor = Dense(self.value_layers_n[1], activation='relu')(self.value_network_executor)\n",
    "        self.value_network_executor = Dense(self.value_layers_n[2], activation='relu')(self.value_network_executor)\n",
    "        \n",
    "        self.logits_executor = Dense(self.action_n)(self.policy_network_executor)\n",
    "        self.values_executor = Dense(1)(self.value_network_executor)\n",
    "        \n",
    "        self.model_executor = Model(inputs=self.inputs_executor, outputs=[self.values_executor, self.logits_executor])\n",
    "        \n",
    "        self.observations = []\n",
    "        self.iter_counter = 0\n",
    "        \n",
    "        self.internal_step_counter_all = 0\n",
    "        self.done_indeed = 0\n",
    "        self.done_done_all = np.zeros(3)\n",
    "        \n",
    "        \n",
    "    def experience_generator(self, i, learner, memorizer):\n",
    "        #def experience_generator(self, i, learner):        \n",
    "        \n",
    "        #print(\"EG BEOFRE WHILE\", i)\n",
    "        \n",
    "        self.worker = i\n",
    "        self.time_start = time.time()\n",
    "        \n",
    "        reload_model_weights = 1\n",
    "        \n",
    "        #while (ray.get(learner.get_episode_counter.remote()) < self.max_episodes) and (ray.get(learner.get_internal_step_counter_best.remote()) < self.internal_step_counter_limit):\n",
    "        #print(\"BEFORE FIRST WHILE\", ray.get(learner.get_episode_counter.remote()), self.max_episodes, self.iter_counter, self.internal_step_counter_limit)\n",
    "        while (ray.get(learner.get_episode_counter.remote()) < self.max_episodes) and (self.iter_counter < self.internal_step_counter_limit):\n",
    "            #print(\"AFTER FIRST WHILE\", ray.get(learner.get_episode_counter.remote()), self.max_episodes, self.iter_counter, self.internal_step_counter_limit)\n",
    "            #print(\"EG AFTER FIRST WHILE\", i)\n",
    "            \n",
    "            self.iter_counter += 1\n",
    "            \n",
    "            #print(\"reload_model_weights\", reload_model_weights)\n",
    "            \n",
    "            if reload_model_weights == 1:\n",
    "                self.model_executor.set_weights(ray.get(learner.get_executor_model.remote()))\n",
    "                reload_model_weights = 0\n",
    "            \n",
    "            #print(\"EG\", self.model_executor.get_weights()[0][0][:5])\n",
    "                \n",
    "            # Collect Examples & Save them in the Central Observation Repository\n",
    "            self.current_state = self.env.reset()\n",
    "            \n",
    "            #print(i, \"STATE AFTER INITIAL RESET\", self.current_state)\n",
    "            \n",
    "            \n",
    "            #print(\"EPISODE CONTER\", ray.get(learner.get_episode_counter.remote()))\n",
    "            # ENSURE EXPLORATION OF advarse STATES\n",
    "            if ray.get(learner.get_episode_counter.remote()) <= 1:\n",
    "                self.prob_advarse_state = self.prob_advarse_state_initial\n",
    "            else:\n",
    "                self.prob_advarse_state = np.clip(self.prob_advarse_state_initial/math.log(ray.get(learner.get_episode_counter.remote()),5), 0.05, 0.2)\n",
    "            \n",
    "            self.prob_random_state = 1-self.prob_advarse_state*4\n",
    "            \n",
    "            # CartPole position_start:\n",
    "            # 0: Close to the Left Edge\n",
    "            # 1: Close to the Right Edge\n",
    "            # 2: Normal, random start (env.restart())\n",
    "            # 3: Leaning Heavilly to the Left\n",
    "            # 4: Leaning Heavilly to the Right\n",
    "            \n",
    "            #print(i, \"self.current_state BEFORE\", self.current_state)\n",
    "            \n",
    "            # Choose one of the 5 scenarios with probabilities defined in p=()\n",
    "            self.pos_start = np.random.choice(5,\n",
    "                                              size = self.num_envs_per_worker, \n",
    "                                              p = (self.prob_advarse_state + self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_advarse_state + self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_random_state,\n",
    "                                              self.prob_advarse_state - self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_advarse_state - self.prob_advarse_state_type_multiplier * self.prob_advarse_state))\n",
    "            \n",
    "            #print(\"self.pos_start\", self.pos_start)\n",
    "            \n",
    "            rows = np.where((self.pos_start == 0) | (self.pos_start == 5))\n",
    "            self.current_state[rows,0] = -1.5 # -2.4 MIN\n",
    "            rows = np.where(self.pos_start == 1)\n",
    "            self.current_state[rows,0] = 1.5 # 2.4 MAX\n",
    "            rows = np.where(self.pos_start == 3)\n",
    "            self.current_state[rows,2] = -0.150 #-0.0.20943951023931953 MIN\n",
    "            rows = np.where(self.pos_start == 4)            \n",
    "            self.current_state[rows,2] = 0.150 #0.0.20943951023931953 MAX            \n",
    "\n",
    "            #print(i, \"self.current_state\", self.current_state)\n",
    "            \n",
    "            #self.env.state = self.current_state\n",
    "            \n",
    "            #print(i, \"self.current_state UPDATED\", self.current_state)\n",
    "            \n",
    "            self.env_id = 0\n",
    "            #print(dir(self.env))\n",
    "            for self.env_indivdual in self.env.envs:\n",
    "                #print(self.env_indivdual.state)\n",
    "                self.env_indivdual.state = self.current_state[self.env_id, :]\n",
    "                #print(self.env_indivdual.state)\n",
    "                self.env_id += 1\n",
    "            #self.env.observations = self.current_state\n",
    "            #self.env.blablabla = self.current_state\n",
    "            \n",
    "            \n",
    "            #print(i, \"self.current_state AFTER\", self.current_state)\n",
    "            #print(i, \"self.env.state\", self.env.state)\n",
    "            #print(i, \"self.env.observatons AFTER\", self.env.observations)\n",
    "            #print(i, \"self.env.blablabla AFTER\", self.env.blablabla)            \n",
    "            \n",
    "            # Custom State Representation Adjustment to help agent learn to be closer to the center\n",
    "            self.current_state = np.append(self.current_state,(self.current_state[:,0] * self.current_state[:,0]).reshape((-1,1)), axis = 1) \n",
    "            \n",
    "            #print(i, \"STATE AFER APPEND\", self.current_state) \n",
    "            \n",
    "            self.observations = np.empty((1,(self.state_n + self.state_n_adj) * 2 + 3, self.num_envs_per_worker))\n",
    "            self.done = np.array([False for i in range(self.num_envs_per_worker)])\n",
    "            self.internal_step_counter = 0\n",
    "            self.collect_obs = []\n",
    "            self.collect_obs_length = []\n",
    "            self.done_indeed = 0\n",
    "            #print(i, \"self.observations INITIAL SHAPE\", self.observations.shape)\n",
    "            \n",
    "            #print(\"self.done BEFORE\", self.done)\n",
    "            #print(i, \"BEFORE SECOND WHILE\", self.done_indeed == 0, self.internal_step_counter <= self.internal_step_counter_limit)\n",
    "            #while (not(self.done.all() == True)) and (self.internal_step_counter <= self.internal_step_counter_limit):\n",
    "            self.done_ind = 0\n",
    "            self.done_ind_new = 0\n",
    "            self.done_done_all = 0\n",
    "            self.internal_step_counter_joint = []\n",
    "            \n",
    "            while (self.done_indeed == 0) and (self.internal_step_counter <= self.internal_step_counter_limit):                \n",
    "                #print(i, \"WITHIN SECOND WHILE\", self.done, self.done_indeed == 0, self.internal_step_counter <= self.internal_step_counter_limit)\n",
    "                #print(i, \"self.current_state 2\", self.current_state, self.current_state.shape)\n",
    "                #self.values, self.logits = self.model_executor(tf.convert_to_tensor(np.array(np.expand_dims(self.current_state,axis=0)), dtype=tf.float32))\n",
    "                self.values, self.logits = self.model_executor(tf.convert_to_tensor(np.array(self.current_state), dtype=tf.float32))\n",
    "                \n",
    "                # EPSILON-GREEDY with DECAY POLICY\n",
    "                \n",
    "                if self.epsilon_decay_policy == 1:\n",
    "                    #epsilon *= (2-epsilon_decay)\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    if self.epsilon >= np.random.rand(): # Random-Informed\n",
    "                        #print(\"self.logits\", self.logits)\n",
    "                        #self.action = np.argmax(self.logits, axis = 2).squeeze()\n",
    "                        self.action = np.argmax(self.logits, axis = 1).squeeze() #######\n",
    "                        #print(\"ACTION\", self.action)\n",
    "                        #action = np.random.choice(action_n)\n",
    "                        #stochastic_action_probabilities = tf.nn.softmax(logits)\n",
    "                        #action = np.random.choice(action_n, p=stochastic_action_probabilities.numpy()[0])\n",
    "                    else: # Greedy\n",
    "                        self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                        #print(\"self.stochastic_action_probabilities.numpy()[0]\",self.stochastic_action_probabilities.numpy()[0])\n",
    "                        \n",
    "                        self.action = np.array([])\n",
    "                        #print(self.action, self.action_n, self.stochastic_action_probabilities.numpy())\n",
    "                        for i in range(self.num_envs_per_worker):\n",
    "                            #self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[0][i])))\n",
    "                            self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[i])))      #########                      \n",
    "                        #print(i, \"self.action 1\", self.action)\n",
    "                        #print(1, self.stochastic_action_probabilities, self.action)\n",
    "                        #action = np.argmax(logits) # Total Collapse\n",
    "                else:\n",
    "                    #self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                    #self.action = np.random.choice(self.action_n, p=self.stochastic_action_probabilities.numpy()[0])\n",
    "                    \n",
    "                    self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                    #print(\"self.stochastic_action_probabilities.numpy()[0]\",self.stochastic_action_probabilities.numpy()[0])\n",
    "\n",
    "                    self.action = np.array([])\n",
    "                    for i in range(self.num_envs_per_worker):\n",
    "                        #self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[0][i])))\n",
    "                        self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[i]))) ########\n",
    "\n",
    "                    #print(2, self.stochastic_action_probabilities, self.action)                        \n",
    "                self.action = self.action.astype(int)\n",
    "            \n",
    "                #print(i, \"self.action 2\", self.action)\n",
    "                #for i in self.action:\n",
    "                #    print(type(i))\n",
    "                \n",
    "                #print(\"self.action\", self.action)\n",
    "                self.next_state, self.reward, self.done, self.info = self.env.step(self.action)\n",
    "\n",
    "                #print(\"NEXT STATE\", self.next_state)\n",
    "                #print(\"STEP 1\", self.next_state, self.reward, self.done, self.info)\n",
    "                \n",
    "                # DOES NOT WORK WITH VECTORIZED ENVIRONMENTS\n",
    "                #self.maybe_end_state = self.env.monitor.flush(force=True)\n",
    "                #print(\"maybe next state\", np.round(self.maybe_end_state,4), np.round(self.next_state,4))\n",
    "                \n",
    "                #print(\"current_state/next_state\", np.round(self.current_state[:,0],4), np.round(self.next_state[:,0],4))\n",
    "                \n",
    "                #print(\"AFTER STEP\", self.done)\n",
    "\n",
    "                #NEXT STEP (3, 4) (3,) (3,) 3\n",
    "                \n",
    "                #print(\"NEXT STEP\", type(self.next_state), type(self.reward), type(self.done), type(self.info))\n",
    "                #print(\"NEXT STEP\", self.next_state.shape, self.reward.shape, self.done.shape, len(self.info))\n",
    "                \n",
    "                #self.next_state = np.append(self.next_state, self.next_state[0] * self.next_state[0])\n",
    "                self.next_state = np.append(self.next_state,(self.next_state[:,0] * self.next_state[:,0]).reshape((-1,1)), axis = 1)\n",
    "                \n",
    "                #print(\"SQUARED\", (self.next_state[:,0] * self.next_state[:,0]).reshape((-1,1)))\n",
    "                #print(\"STEP 2\", self.next_state, self.reward, self.done, self.info)                \n",
    "\n",
    "                #print(\"self.next_state\", self.next_state)\n",
    "                # Add desired-behaviour incentive to the reward function\n",
    "                self.R_pos = 1*(1-np.abs(self.next_state[:, 0])/2.4) # 2.4 max value ### !!! in documentation it says 4.8 but failes beyound 2.4\n",
    "                self.R_ang = 1*(1-np.abs(self.next_state[:, 2])/0.20943951023931953) ### !!! in documentation it says 0.418 max value\n",
    "\n",
    "                #print(\"SHAPES BEFORE\", self.reward.shape, self.R_pos.shape, self.R_ang.shape)\n",
    "                #print(\"REWARD BEFORE\", self.reward, self.R_pos, self.R_ang)\n",
    "                self.reward = self.reward + self.R_pos + self.R_ang\n",
    "                \n",
    "                #print(\"self.reward AFTER\", self.reward)\n",
    "                \n",
    "                # Custom Fail Reward to speed up Learning of conseqences of being in advarse position\n",
    "                #print(\"self.done\", self.done)\n",
    "                \n",
    "                rows = np.where(self.done == True)\n",
    "                \n",
    "                #print(\"BEFORE self.reward NEGATIVE\", self.reward, rows)\n",
    "                self.reward[rows] = self.reward_negative # ST Original -1\n",
    "                #self.reward[rows-1] = self.reward_negative # ST Original -1\n",
    "                #print(\"AFTER self.reward NEGATIVE\", self.reward, rows)                \n",
    "                #if self.done == True: \n",
    "                #    self.reward = self.reward_negative # ST Original -1\n",
    "                        \n",
    "                #current_observation = np.append(current_state,(reward, done, action))\n",
    "                \n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 0 (3, 5) (16,)\n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 1 (31,) (3,) (3,) (3,)\n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 2 (1, 13, 3) (40,)\n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 0\", self.current_state.shape, self.next_state.shape)\n",
    "                \n",
    "                #print(\"BEFORE STACKING\", self.current_state, self.next_state)\n",
    "                #self.current_observation = np.append(self.current_state, self.next_state)\n",
    "                self.current_observation = np.concatenate([self.current_state, self.next_state], axis = 1)\n",
    "                #print(\"AFTER STACKING\", self.current_observation)\n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 1\", self.current_observation.shape, self.reward.shape, self.done.shape, self.action.shape)\n",
    "                #print(\"BEFORE STACKING 1\", self.current_observation, self.reward, self.done, self.action)\n",
    "                \n",
    "                #self.current_observation = np.append(self.current_observation,(self.reward, self.done, self.action))\n",
    "                self.current_observation = np.concatenate([self.current_observation, self.reward.reshape([-1,1]), self.done.reshape([-1,1]), self.action.reshape([-1,1])], axis = 1)\n",
    "                \n",
    "                #print(\"AFTER STACKING 1\", self.current_observation)\n",
    "                \n",
    "                #print(\"BEFORE RESHAPE\", self.current_observation)\n",
    "                #print(\"AFTER RESHAPE\", self.current_observation.reshape([1,13,self.num_envs_per_worker]))\n",
    "                #print(\"EXPAND DIM\", np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0))\n",
    "                \n",
    "                #print(\"BEFORE RESHAPE\", self.current_observation.shape)\n",
    "                #print(\"AFTER RESHAPE\", self.current_observation.reshape([1,13,self.num_envs_per_worker]).shape)\n",
    "                #print(\"EXPAND DIM\", np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0).shape)                \n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 2\", self.observations.shape, self.current_observation.shape, self.current_observation[0])\n",
    "                #self.observations = np.vstack((self.observations, self.current_observation.reshape([1,13,self.num_envs_per_worker])))\n",
    "                \n",
    "                #print(\"BEFORE SWAPPING\", self.current_observation)\n",
    "                self.observations = np.vstack((self.observations, np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0)))\n",
    "                #print(\"AFTER SWAPPING\", self.observations)\n",
    "                \n",
    "                if (self.observations.shape[0] > 1) and ((self.observations.shape[0] % self.experience_max_batch_size) == 0):\n",
    "                    self.observations = self.observations[-self.experience_max_batch_size:,:,:]\n",
    "                #print(\"SHAPES AFTER STACKING \", self.observations.shape, self.observations[:,:,0])\n",
    "\n",
    "                self.current_state = self.next_state\n",
    "                self.internal_step_counter += 1\n",
    "                  \n",
    "                if self.internal_step_counter == 1:\n",
    "                    self.observations = self.observations[1:,:,:]\n",
    "                    #print(\"self.observations\",self.observations.shape, self.observations)\n",
    "                \n",
    "                #print(\"self.done END\", self.done)\n",
    "                \n",
    "                if self.done.any() == True:\n",
    "                    self.done_ind = self.done * 1\n",
    "                    #print(\"self.done_ind\", self.done_ind, self.done_ind.sum(), self.done_done_all)\n",
    "                    self.done_ind_new = np.where((self.done_ind - self.done_done_all) == 1)\n",
    "                    #print(\"self.done_ind_new\", self.done_ind_new, self.done_ind_new[0].shape)\n",
    "                    \n",
    "                    if self.done_ind_new[0].shape[0] > 0:\n",
    "                        for exp in self.done_ind_new[0]:\n",
    "                            \n",
    "                            #print(\"exp\", exp)\n",
    "                            self.observations_current = self.observations[:,:,exp]\n",
    "                            self.terminal_state = self.info[exp][\"terminal_observation\"]\n",
    "                            #print(\"1 CURRENT STATE [-1]\", self.observations_current[-1], \"TERMINAL STATE\", self.terminal_state)\n",
    "                            self.terminal_state = np.append(self.terminal_state, self.terminal_state[0] * self.terminal_state[0])\n",
    "                            #print(\"self.terminal_state SHAPE\", self.terminal_state.shape)\n",
    "                            self.observations_current[-1,5:10] = self.terminal_state\n",
    "                            \n",
    "                            #self.observations_current = self.observations_current[1:,:]\n",
    "                            \n",
    "                            #print(\"CURRENT STATE\", self.observations_current, self.done)\n",
    "                            #print(\"self.info\", self.info)\n",
    "                            #print(\"CURRENT STATE [-1]\", self.observations_current[-1])\n",
    "                            #print(\"2 CURRENT STATE [-1]\", self.observations_current[-1], \"TERMINAL STATE\", self.terminal_state)\n",
    "\n",
    "                            self.collect_obs_length.append(self.observations_current.shape[0]) ### WRONG as now max length is self.experience_max_batch_size\n",
    "                            #print(\"self.collect_obs_length LEN()\", len(self.collect_obs_length), \"self.observations_current.shape[0]\", self.observations_current.shape[0])\n",
    "\n",
    "                            self.observations_current = self.observations_current[-np.minimum(self.observations_current.shape[0], self.experience_max_batch_size):]\n",
    "                            #print(\"self.observations_current\", self.observations_current)\n",
    "\n",
    "                            self.exp_len = self.observations_current.shape[0]\n",
    "                            #print(\"self.exp_len\", self.exp_len)\n",
    "                            self.exp_indices = np.array(range(self.exp_len)) + 1\n",
    "                            #print()\n",
    "                            self.rewards = np.flip(self.observations_current[:,(self.state_n + self.state_n_adj) * 2 ])\n",
    "                            \n",
    "                            # !!!!!!!! POTENTIAL NEGATIVE REWARD UPDATE !!!!!!!!\n",
    "                            #self.reward[1] = self.reward_negative\n",
    "                            \n",
    "                            #print(\"selfrewards\",self.rewards)\n",
    "                            self.discounted_rewards = np.empty(self.exp_len)\n",
    "                            #print(\"self.discounted_rewards\", self.discounted_rewards)\n",
    "                            self.reward_sum = 0\n",
    "\n",
    "                            if self.observations_current[-1,-2] == 0:\n",
    "                                # IN CASE THE EPISODE HAS NTO TERMINATED\n",
    "                                self.observations_current[-1,-2] = 2                        \n",
    "                                self.gamma = np.full(self.exp_len, 0.99)\n",
    "                                #print(1)\n",
    "                            else:\n",
    "                                # IN CASE THE EPISODE HAS TERMINATED\n",
    "                                #print(2)\n",
    "                                #print(\"exp_indices\", exp_indices)\n",
    "                                self.gamma = np.clip(0.0379 * np.log(self.exp_indices-1) + 0.7983, 0.5, 0.99)\n",
    "                            #print(\"END SHAPE\", self.observations.shape)    \n",
    "                            if self.observations_current[-1,-2] == 1:\n",
    "                                self.gamma[0] = 1\n",
    "                                #print(3)\n",
    "\n",
    "                            for step in range(self.exp_len):\n",
    "                                self.reward_sum = self.rewards[step] + self.gamma[step] * self.reward_sum\n",
    "                                self.discounted_rewards[step] = self.reward_sum    \n",
    "\n",
    "                            self.discounted_rewards = np.flip(self.discounted_rewards)\n",
    "                            \n",
    "                            #print(\"BEFORE self.observations_current\", np.round(np.float32(self.observations_current),4))\n",
    "                            #print(\"BEFORE self.discounted_rewards\", np.round(np.float32(self.discounted_rewards),4))\n",
    "\n",
    "                            self.observations_current = np.hstack((self.observations_current, np.expand_dims(self.discounted_rewards, axis = 1)))\n",
    "                            #print(\"AFTER self.observations_current\", np.round(np.float32(self.observations_current),4))                            \n",
    "                            self.observations_current = np.hstack((self.observations_current, np.zeros((self.observations_current.shape[0], self.pmb_cols))))\n",
    "\n",
    "                            #print(self.worker, ray.get(learner.get_episode_counter.remote()), \"observations\", self.observations_current.shape)\n",
    "                            \n",
    "                            #print(self.observations_current.shape, np.round(self.observations_current,4))\n",
    "                            \n",
    "                            # !!!!!!!! POTENTIAL NEGATIVE REWARD UPDATE !!!!!!!!\n",
    "                            #self.collect_obs.append(self.observations_current[:-1,:])\n",
    "                            \n",
    "                            \n",
    "                            self.collect_obs.append(self.observations_current)\n",
    "                            \n",
    "                            #print(\"EXECUTOR - self.collect_obs LEN\", len(self.collect_obs), \"self.observations_current.shape\", self.observations_current.shape)\n",
    "                            \n",
    "                            self.internal_step_counter_all += self.internal_step_counter\n",
    "                            \n",
    "\n",
    "                            #print(\"BEFORE\", self.done_done_all)\n",
    "                            self.done_done_all = np.maximum(self.done_done_all, self.done_ind)\n",
    "                            \n",
    "                            self.internal_step_counter_joint.append(self.internal_step_counter)\n",
    "\n",
    "                            #print(\"AFTER\", self.done_done_all, self.done_done_all.sum(), self.num_envs_per_worker)\n",
    "                            \n",
    "                            ray.get(learner.increase_episode_counter.remote())                                                     \n",
    "\n",
    "                        if self.done_done_all.sum() == self.num_envs_per_worker:\n",
    "\n",
    "                            #print(\"1\")\n",
    "                            self.done_indeed = 1\n",
    "                            ray.get(learner.increase_executor_counter.remote())                                \n",
    "\n",
    "                            #print(\"self.collect_obs\", self.collect_obs)\n",
    "\n",
    "                            #print(\"2\")\n",
    "\n",
    "                            # Update Counters to Track Progress\n",
    "\n",
    "\n",
    "                            #self.internal_step_counter_all += self.internal_step_counter\n",
    "\n",
    "\n",
    "                            #print(\"3\")\n",
    "                            if self.internal_step_counter_all < self.experience_batch_size:\n",
    "                                pass\n",
    "                            else:\n",
    "                                self.internal_step_counter_all = 0  \n",
    "                                ray.get(learner.increase_executor_counter.remote())\n",
    "\n",
    "                            print(\"Ending Executor:\", self.worker, \"Episode\", ray.get(learner.get_episode_counter.remote()), \"Initial State\", self.pos_start, \"Steps:\", self.internal_step_counter_joint)\n",
    "\n",
    "                            #print(\"4\")\n",
    "                            if self.internal_step_counter >= ray.get(learner.get_internal_step_counter_best.remote()):\n",
    "                                learner.set_internal_step_counter_best.remote(self.internal_step_counter)\n",
    "\n",
    "                                print(\"############################## BEST EPISODE LENGTH:\", self.internal_step_counter, \"Executor:\", self.worker)       \n",
    "\n",
    "                            #print(\"5\")\n",
    "                            if ray.get(learner.get_internal_step_counter_best.remote()) >= self.internal_step_counter_limit:\n",
    "\n",
    "                                self.episod_counter_target = ray.get(learner.get_episode_counter.remote())\n",
    "\n",
    "                                print(\"\\nREACHED GOAL of\", self.internal_step_counter_limit,  \"Steps in\", self.episod_counter_target, \"episodes; Learning Iterations (Not Available); in\",time.time()-self.time_start, \"seconds \\n\")  \n",
    "\n",
    "                            #ray.get(memorizer.collect.remote(learner, self.collect_obs))\n",
    "                            ray.get(memorizer.collect.remote(learner, self.collect_obs))                             \n",
    "                            ray.get(memorizer.collect_length.remote(self.collect_obs_length))\n",
    "                            \n",
    "                            self.collect_obs = []\n",
    "                            self.collect_obs_length = []                                \n",
    "\n",
    "                            #print(\"6\")\n",
    "                            while ray.get(learner.get_executor_counter.remote()) >= self.num_envs_per_worker:\n",
    "                                #print(\"WAITING FOR A NEW MODEL\")\n",
    "                                pass\n",
    "\n",
    "                            reload_model_weights = 1\n",
    "                        \n",
    "            #self.observations = np.empty((1,(self.state_n + self.state_n_adj) * 2 + 3, self.num_envs_per_worker))\n",
    "            #self.done_indeed = 0\n",
    "                \n",
    "\n",
    "@ray.remote\n",
    "def train(i, memorizer, learner, args):\n",
    "    #@ray.remote\n",
    "    #def train(i, learner):    \n",
    "    #executor = Executor.remote(memorizer, learner)\n",
    "    executor = Executor.remote(i, args)\n",
    "    ray.get(executor.experience_generator.remote(i, learner, memorizer))\n",
    "    #executor.experience_generator.remote(i, learner, memorizer)\n",
    "    \n",
    "    return \"DONE! \" + str(i)\n",
    "    \n",
    "learner = Learner.remote()\n",
    "memorizer = Memorizer.remote(learner)\n",
    "#executor = Executor.remote(memorizer, learner)    \n",
    "\n",
    "time_start = time.time()\n",
    "results = ray.get([train.remote(i, memorizer, learner, args) for i in range(int(args[\"executors_n\"]))])\n",
    "#results = ray.get([train.remote(i, learner) for i in range(int(cores))])\n",
    "print(\"RESULTS\", results, time.time()-time_start)\n",
    "    \n",
    "#print(1, ray.get(learner.get_episode_counter.remote()))\n",
    "#ray.get(learner.reset_episode_counter.remote())\n",
    "#print(2, ray.get(learner.get_episode_counter.remote()))\n",
    "\n",
    "# Do function from which to run it and initialize executors within each instance of the function.\n",
    "# within executor run memorizer collect function that will collect experience to the moemory buffer and check if enough experience has been collected and if so it will run the memorization proces - possibly memorization process can be coded in a separate fucntion\n",
    "\n",
    "\n",
    "print(ray.get(learner.get_executor_counter.remote()))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e25051",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.5160507,  0.48394936],\n",
    "               [0.5007256,  0.49927434],\n",
    "               [0.500383,   0.49961698]])\n",
    "\n",
    "print(a.shape)\n",
    "action = np.argmax(a, axis = 1).squeeze()\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. + 0.9989191 + 0.907359 , 1. + 0.9832693+0.971138  , 1. + 0.9925494+0.76985335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1.05557414e-02 -2.88285837e-02 -2.04720601e-01 -4.39759582e-01\n",
    "  1.11423673e-04 -1.11323139e-02 -2.20554456e-01 -2.13515788e-01\n",
    " -2.17944577e-01  1.23928417e-04 -1.00000000e+01  1.00000000e+00\n",
    "  0.00000000e+00 -1.00000000e+01  0.00000000e+00  0.00000000e+00\n",
    "  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
    "\n",
    "[ 1.37023389e-01,  4.17018294e-01, -2.09414169e-01, -9.43465173e-01,\n",
    "  1.87754091e-02,  2.76441611e-02, -2.52805892e-02,  1.23133240e-02,\n",
    "  4.92334403e-02, 7.64199649e-04, -1.50000000e+01,  1.00000000e+00,\n",
    "  1.00000000e+00, -1.50000000e+01,  0.00000000e+00, 0.00000000e+00,\n",
    "  0.00000000e+00,  0.00000000e+00, 0.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "current_state = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    next_state, reward, done, info = env.step(1)\n",
    "    print(current_state / next_state, done, current_state, next_state, env_vect.observations)\n",
    "    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc0bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE 1 <class 'numpy.ndarray'>\n",
      "current_state [[ 0.0365  0.0361  0.0384  0.0498]\n",
      " [ 0.0088 -0.0075 -0.0408 -0.0221]\n",
      " [-0.0093  0.0404  0.0338  0.0433]]\n",
      "self.env.state [[ 0.0365  0.0361  0.0384  0.0498]\n",
      " [ 0.0088 -0.0075 -0.0408 -0.0221]\n",
      " [-0.0093  0.0404  0.0338  0.0433]]\n",
      "TYPE 2 <class 'numpy.ndarray'>\n",
      "env_vect.observations [[-10.       0.4157  -0.0285  -0.6019]\n",
      " [ 10.       0.3979  -0.0239  -0.6019]\n",
      " [ 10.       0.8187  -0.072   -1.2578]]\n",
      "0\n",
      "0 [[ 0.0372  0.2306  0.0394 -0.2305]\n",
      " [ 0.0086  0.1882 -0.0413 -0.3274]\n",
      " [-0.0085  0.2351  0.0347 -0.2385]] [[ 0.0372  0.2306  0.0394 -0.2305]\n",
      " [ 0.0086  0.1882 -0.0413 -0.3274]\n",
      " [-0.0085  0.2351  0.0347 -0.2385]] [False False False] [{}, {}, {}]\n",
      "1\n",
      "1 [[ 0.0418  0.4252  0.0348 -0.5105]\n",
      " [ 0.0124  0.3839 -0.0478 -0.6328]\n",
      " [-0.0038  0.4297  0.0299 -0.5201]] [[ 0.0418  0.4252  0.0348 -0.5105]\n",
      " [ 0.0124  0.3839 -0.0478 -0.6328]\n",
      " [-0.0038  0.4297  0.0299 -0.5201]] [False False False] [{}, {}, {}]\n",
      "2\n",
      "2 [[ 0.0503  0.6198  0.0246 -0.792 ]\n",
      " [ 0.0201  0.5796 -0.0605 -0.9402]\n",
      " [ 0.0048  0.6244  0.0195 -0.8032]] [[ 0.0503  0.6198  0.0246 -0.792 ]\n",
      " [ 0.0201  0.5796 -0.0605 -0.9402]\n",
      " [ 0.0048  0.6244  0.0195 -0.8032]] [False False False] [{}, {}, {}]\n",
      "3\n",
      "3 [[ 0.0627  0.8146  0.0087 -1.0769]\n",
      " [ 0.0317  0.7755 -0.0793 -1.2512]\n",
      " [ 0.0173  0.8192  0.0034 -1.0897]] [[ 0.0627  0.8146  0.0087 -1.0769]\n",
      " [ 0.0317  0.7755 -0.0793 -1.2512]\n",
      " [ 0.0173  0.8192  0.0034 -1.0897]] [False False False] [{}, {}, {}]\n",
      "4\n",
      "4 [[ 0.079   1.0096 -0.0128 -1.3668]\n",
      " [ 0.0472  0.9715 -0.1043 -1.5677]\n",
      " [ 0.0337  1.0143 -0.0184 -1.3813]] [[ 0.079   1.0096 -0.0128 -1.3668]\n",
      " [ 0.0472  0.9715 -0.1043 -1.5677]\n",
      " [ 0.0337  1.0143 -0.0184 -1.3813]] [False False False] [{}, {}, {}]\n",
      "5\n",
      "5 [[ 0.0992  1.2049 -0.0401 -1.6634]\n",
      " [ 0.0666  1.1677 -0.1357 -1.891 ]\n",
      " [ 0.054   1.2096 -0.046  -1.6797]] [[ 0.0992  1.2049 -0.0401 -1.6634]\n",
      " [ 0.0666  1.1677 -0.1357 -1.891 ]\n",
      " [ 0.054   1.2096 -0.046  -1.6797]] [False False False] [{}, {}, {}]\n",
      "6\n",
      "6 [[ 0.1233  1.4004 -0.0734 -1.9683]\n",
      " [ 0.09    1.3641 -0.1735 -2.2225]\n",
      " [ 0.0782  1.4052 -0.0796 -1.9863]] [[ 0.1233  1.4004 -0.0734 -1.9683]\n",
      " [ 0.09    1.3641 -0.1735 -2.2225]\n",
      " [ 0.0782  1.4052 -0.0796 -1.9863]] [False False False] [{}, {}, {}]\n",
      "7\n",
      "7 [[ 0.1513  1.5962 -0.1128 -2.2828]\n",
      " [-0.0217  0.0429 -0.0092  0.0443]\n",
      " [ 0.1063  1.6011 -0.1193 -2.3025]] [[ 0.1513  1.5962 -0.1128 -2.2828]\n",
      " [-0.0217  0.0429 -0.0092  0.0443]\n",
      " [ 0.1063  1.6011 -0.1193 -2.3025]] [False  True False] [{}, {'actual_next_step': array([ 0.1172,  1.5604, -0.2179, -2.5633], dtype=float32), 'terminal_observation': array([ 0.1172,  1.5604, -0.2179, -2.5633], dtype=float32)}, {}]\n",
      "8\n",
      "8 [[ 0.1832  1.7922 -0.1584 -2.608 ]\n",
      " [-0.0209  0.2381 -0.0083 -0.2513]\n",
      " [ 0.1383  1.7971 -0.1654 -2.6294]] [[ 0.1832  1.7922 -0.1584 -2.608 ]\n",
      " [-0.0209  0.2381 -0.0083 -0.2513]\n",
      " [ 0.1383  1.7971 -0.1654 -2.6294]] [False False False] [{}, {}, {}]\n",
      "9\n",
      "9 [[ 0.0119 -0.0271 -0.0358 -0.0206]\n",
      " [-0.0161  0.4334 -0.0133 -0.5466]\n",
      " [-0.032   0.0272 -0.0419 -0.0432]] [[ 0.0119 -0.0271 -0.0358 -0.0206]\n",
      " [-0.0161  0.4334 -0.0133 -0.5466]\n",
      " [-0.032   0.0272 -0.0419 -0.0432]] [ True False  True] [{'actual_next_step': array([ 0.2191,  1.9882, -0.2106, -2.9446], dtype=float32), 'terminal_observation': array([ 0.2191,  1.9882, -0.2106, -2.9446], dtype=float32)}, {}, {'actual_next_step': array([ 0.1742,  1.9931, -0.2179, -2.9678], dtype=float32), 'terminal_observation': array([ 0.1742,  1.9931, -0.2179, -2.9678], dtype=float32)}]\n",
      "10\n",
      "10 [[ 0.0114  0.1685 -0.0362 -0.3244]\n",
      " [-0.0074  0.6287 -0.0242 -0.8434]\n",
      " [-0.0315  0.2229 -0.0428 -0.3488]] [[ 0.0114  0.1685 -0.0362 -0.3244]\n",
      " [-0.0074  0.6287 -0.0242 -0.8434]\n",
      " [-0.0315  0.2229 -0.0428 -0.3488]] [False False False] [{}, {}, {}]\n",
      "11\n",
      "11 [[ 0.0147  0.3641 -0.0427 -0.6283]\n",
      " [ 0.0052  0.8241 -0.0411 -1.1436]\n",
      " [-0.027   0.4186 -0.0498 -0.6547]] [[ 0.0147  0.3641 -0.0427 -0.6283]\n",
      " [ 0.0052  0.8241 -0.0411 -1.1436]\n",
      " [-0.027   0.4186 -0.0498 -0.6547]] [False False False] [{}, {}, {}]\n",
      "12\n",
      "12 [[ 0.022   0.5598 -0.0553 -0.9341]\n",
      " [ 0.0216  1.0197 -0.064  -1.4489]\n",
      " [-0.0186  0.6143 -0.0628 -0.9626]] [[ 0.022   0.5598 -0.0553 -0.9341]\n",
      " [ 0.0216  1.0197 -0.064  -1.4489]\n",
      " [-0.0186  0.6143 -0.0628 -0.9626]] [False False False] [{}, {}, {}]\n",
      "13\n",
      "13 [[ 0.0332  0.7556 -0.0739 -1.2436]\n",
      " [ 0.042   1.2156 -0.093  -1.7609]\n",
      " [-0.0063  0.8102 -0.0821 -1.2744]] [[ 0.0332  0.7556 -0.0739 -1.2436]\n",
      " [ 0.042   1.2156 -0.093  -1.7609]\n",
      " [-0.0063  0.8102 -0.0821 -1.2744]] [False False False] [{}, {}, {}]\n",
      "14\n",
      "14 [[ 0.0483  0.9516 -0.0988 -1.5585]\n",
      " [ 0.0663  1.4116 -0.1282 -2.081 ]\n",
      " [ 0.0099  1.0063 -0.1076 -1.5916]] [[ 0.0483  0.9516 -0.0988 -1.5585]\n",
      " [ 0.0663  1.4116 -0.1282 -2.081 ]\n",
      " [ 0.0099  1.0063 -0.1076 -1.5916]] [False False False] [{}, {}, {}]\n",
      "15\n",
      "15 [[ 0.0673  1.1478 -0.13   -1.8803]\n",
      " [ 0.0946  1.6078 -0.1698 -2.4104]\n",
      " [ 0.03    1.2025 -0.1394 -1.9158]] [[ 0.0673  1.1478 -0.13   -1.8803]\n",
      " [ 0.0946  1.6078 -0.1698 -2.4104]\n",
      " [ 0.03    1.2025 -0.1394 -1.9158]] [False False False] [{}, {}, {}]\n",
      "16\n",
      "16 [[ 0.0903  1.3441 -0.1676 -2.2104]\n",
      " [-0.0257 -0.0277 -0.0001 -0.0191]\n",
      " [ 0.054   1.3989 -0.1777 -2.2483]] [[ 0.0903  1.3441 -0.1676 -2.2104]\n",
      " [-0.0257 -0.0277 -0.0001 -0.0191]\n",
      " [ 0.054   1.3989 -0.1777 -2.2483]] [False  True False] [{}, {'actual_next_step': array([ 0.1267,  1.8039, -0.218 , -2.75  ], dtype=float32), 'terminal_observation': array([ 0.1267,  1.8039, -0.218 , -2.75  ], dtype=float32)}, {}]\n",
      "17\n",
      "17 [[-0.013   0.0235 -0.0482 -0.0096]\n",
      " [-0.0263  0.1674 -0.0005 -0.3118]\n",
      " [ 0.0477  0.0123  0.0154  0.0146]] [[-0.013   0.0235 -0.0482 -0.0096]\n",
      " [-0.0263  0.1674 -0.0005 -0.3118]\n",
      " [ 0.0477  0.0123  0.0154  0.0146]] [ True False  True] [{'actual_next_step': array([ 0.1172,  1.5404, -0.2118, -2.5497], dtype=float32), 'terminal_observation': array([ 0.1172,  1.5404, -0.2118, -2.5497], dtype=float32)}, {}, {'actual_next_step': array([ 0.082 ,  1.5951, -0.2227, -2.59  ], dtype=float32), 'terminal_observation': array([ 0.082 ,  1.5951, -0.2227, -2.59  ], dtype=float32)}]\n",
      "18\n",
      "18 [[-0.0125  0.2193 -0.0484 -0.3171]\n",
      " [-0.023   0.3625 -0.0067 -0.6046]\n",
      " [ 0.0479  0.2072  0.0157 -0.2731]] [[-0.0125  0.2193 -0.0484 -0.3171]\n",
      " [-0.023   0.3625 -0.0067 -0.6046]\n",
      " [ 0.0479  0.2072  0.0157 -0.2731]] [False False False] [{}, {}, {}]\n",
      "19\n",
      "19 [[-0.0081  0.4151 -0.0548 -0.6247]\n",
      " [-0.0157  0.5578 -0.0188 -0.8994]\n",
      " [ 0.0521  0.4021  0.0103 -0.5608]] [[-0.0081  0.4151 -0.0548 -0.6247]\n",
      " [-0.0157  0.5578 -0.0188 -0.8994]\n",
      " [ 0.0521  0.4021  0.0103 -0.5608]] [False False False] [{}, {}, {}]\n",
      "20\n",
      "20 [[ 0.0002  0.6109 -0.0673 -0.9341]\n",
      " [-0.0045  0.7531 -0.0368 -1.1979]\n",
      " [ 0.0601  0.5971 -0.001  -0.8503]] [[ 0.0002  0.6109 -0.0673 -0.9341]\n",
      " [-0.0045  0.7531 -0.0368 -1.1979]\n",
      " [ 0.0601  0.5971 -0.001  -0.8503]] [False False False] [{}, {}, {}]\n",
      "21\n",
      "21 [[ 0.0124  0.8069 -0.086  -1.2471]\n",
      " [ 0.0105  0.9487 -0.0607 -1.5019]\n",
      " [ 0.072   0.7922 -0.018  -1.1432]] [[ 0.0124  0.8069 -0.086  -1.2471]\n",
      " [ 0.0105  0.9487 -0.0607 -1.5019]\n",
      " [ 0.072   0.7922 -0.018  -1.1432]] [False False False] [{}, {}, {}]\n",
      "22\n",
      "22 [[ 0.0285  1.003  -0.1109 -1.5655]\n",
      " [ 0.0295  1.1445 -0.0908 -1.8129]\n",
      " [ 0.0879  0.9876 -0.0408 -1.4415]] [[ 0.0285  1.003  -0.1109 -1.5655]\n",
      " [ 0.0295  1.1445 -0.0908 -1.8129]\n",
      " [ 0.0879  0.9876 -0.0408 -1.4415]] [False False False] [{}, {}, {}]\n",
      "23\n",
      "23 [[ 0.0486  1.1993 -0.1422 -1.8906]\n",
      " [ 0.0524  1.3405 -0.127  -2.1324]\n",
      " [ 0.1076  1.1832 -0.0697 -1.7467]] [[ 0.0486  1.1993 -0.1422 -1.8906]\n",
      " [ 0.0524  1.3405 -0.127  -2.1324]\n",
      " [ 0.1076  1.1832 -0.0697 -1.7467]] [False False False] [{}, {}, {}]\n",
      "24\n",
      "24 [[ 0.0726  1.3956 -0.18   -2.2238]\n",
      " [ 0.0792  1.5366 -0.1697 -2.4614]\n",
      " [ 0.1313  1.379  -0.1046 -2.0602]] [[ 0.0726  1.3956 -0.18   -2.2238]\n",
      " [ 0.0792  1.5366 -0.1697 -2.4614]\n",
      " [ 0.1313  1.379  -0.1046 -2.0602]] [False False False] [{}, {}, {}]\n",
      "25\n",
      "25 [[ 0.0446  0.0225 -0.0209 -0.0084]\n",
      " [ 0.0368 -0.002  -0.0274 -0.0439]\n",
      " [ 0.1589  1.5751 -0.1458 -2.3833]] [[ 0.0446  0.0225 -0.0209 -0.0084]\n",
      " [ 0.0368 -0.002  -0.0274 -0.0439]\n",
      " [ 0.1589  1.5751 -0.1458 -2.3833]] [ True  True False] [{'actual_next_step': array([ 0.1005,  1.592 , -0.2245, -2.5662], dtype=float32), 'terminal_observation': array([ 0.1005,  1.592 , -0.2245, -2.5662], dtype=float32)}, {'actual_next_step': array([ 0.1099,  1.7327, -0.2189, -2.801 ], dtype=float32), 'terminal_observation': array([ 0.1099,  1.7327, -0.2189, -2.801 ], dtype=float32)}, {}]\n",
      "26\n",
      "26 [[ 0.045   0.2179 -0.021  -0.3076]\n",
      " [ 0.0368  0.1935 -0.0282 -0.3451]\n",
      " [ 0.1904  1.7711 -0.1935 -2.717 ]] [[ 0.045   0.2179 -0.021  -0.3076]\n",
      " [ 0.0368  0.1935 -0.0282 -0.3451]\n",
      " [ 0.1904  1.7711 -0.1935 -2.717 ]] [False False False] [{}, {}, {}]\n",
      "27\n",
      "27 [[ 0.0494  0.4133 -0.0272 -0.6068]\n",
      " [ 0.0407  0.389  -0.0351 -0.6465]\n",
      " [-0.0072  0.0465 -0.0474  0.0159]] [[ 0.0494  0.4133 -0.0272 -0.6068]\n",
      " [ 0.0407  0.389  -0.0351 -0.6465]\n",
      " [-0.0072  0.0465 -0.0474  0.0159]] [False False  True] [{}, {}, {'actual_next_step': array([ 0.2258,  1.9671, -0.2478, -3.0619], dtype=float32), 'terminal_observation': array([ 0.2258,  1.9671, -0.2478, -3.0619], dtype=float32)}]\n",
      "28\n",
      "28 [[ 0.0577  0.6088 -0.0393 -0.9079]\n",
      " [ 0.0484  0.5846 -0.0481 -0.9501]\n",
      " [-0.0063  0.2422 -0.0471 -0.2913]] [[ 0.0577  0.6088 -0.0393 -0.9079]\n",
      " [ 0.0484  0.5846 -0.0481 -0.9501]\n",
      " [-0.0063  0.2422 -0.0471 -0.2913]] [False False False] [{}, {}, {}]\n",
      "29\n",
      "29 [[ 0.0698  0.8044 -0.0575 -1.2127]\n",
      " [ 0.0601  0.7804 -0.0671 -1.2575]\n",
      " [-0.0015  0.438  -0.053  -0.5985]] [[ 0.0698  0.8044 -0.0575 -1.2127]\n",
      " [ 0.0601  0.7804 -0.0671 -1.2575]\n",
      " [-0.0015  0.438  -0.053  -0.5985]] [False False False] [{}, {}, {}]\n",
      "30\n",
      "30 [[ 0.0859  1.0002 -0.0817 -1.5228]\n",
      " [ 0.0757  0.9763 -0.0922 -1.5704]\n",
      " [ 0.0073  0.6338 -0.0649 -0.9074]] [[ 0.0859  1.0002 -0.0817 -1.5228]\n",
      " [ 0.0757  0.9763 -0.0922 -1.5704]\n",
      " [ 0.0073  0.6338 -0.0649 -0.9074]] [False False False] [{}, {}, {}]\n",
      "31\n",
      "31 [[ 0.1059  1.1963 -0.1122 -1.8399]\n",
      " [ 0.0953  1.1724 -0.1236 -1.8903]\n",
      " [ 0.02    0.8297 -0.0831 -1.2197]] [[ 0.1059  1.1963 -0.1122 -1.8399]\n",
      " [ 0.0953  1.1724 -0.1236 -1.8903]\n",
      " [ 0.02    0.8297 -0.0831 -1.2197]] [False False False] [{}, {}, {}]\n",
      "32\n",
      "32 [[ 0.1299  1.3924 -0.149  -2.1652]\n",
      " [ 0.1187  1.3686 -0.1614 -2.2187]\n",
      " [ 0.0366  1.0258 -0.1075 -1.5373]] [[ 0.1299  1.3924 -0.149  -2.1652]\n",
      " [ 0.1187  1.3686 -0.1614 -2.2187]\n",
      " [ 0.0366  1.0258 -0.1075 -1.5373]] [False False False] [{}, {}, {}]\n",
      "33\n",
      "33 [[ 0.1577  1.5887 -0.1923 -2.4999]\n",
      " [ 0.1461  1.5649 -0.2058 -2.5565]\n",
      " [ 0.0571  1.2221 -0.1382 -1.8615]] [[ 0.1577  1.5887 -0.1923 -2.4999]\n",
      " [ 0.1461  1.5649 -0.2058 -2.5565]\n",
      " [ 0.0571  1.2221 -0.1382 -1.8615]] [False False False] [{}, {}, {}]\n",
      "34\n",
      "34 [[-0.0153 -0.0321 -0.0037  0.024 ]\n",
      " [ 0.0494  0.0082  0.0294 -0.0403]\n",
      " [ 0.0815  1.4184 -0.1754 -2.1937]] [[-0.0153 -0.0321 -0.0037  0.024 ]\n",
      " [ 0.0494  0.0082  0.0294 -0.0403]\n",
      " [ 0.0815  1.4184 -0.1754 -2.1937]] [ True  True False] [{'actual_next_step': array([ 0.1895,  1.7848, -0.2423, -2.8449], dtype=float32), 'terminal_observation': array([ 0.1895,  1.7848, -0.2423, -2.8449], dtype=float32)}, {'actual_next_step': array([ 0.1774,  1.7609, -0.2569, -2.9045], dtype=float32), 'terminal_observation': array([ 0.1774,  1.7609, -0.2569, -2.9045], dtype=float32)}, {}]\n",
      "35\n",
      "35 [[-0.0159  0.1631 -0.0032 -0.2699]\n",
      " [ 0.0496  0.2028  0.0286 -0.3235]\n",
      " [ 0.045  -0.0008 -0.0308  0.0352]] [[-0.0159  0.1631 -0.0032 -0.2699]\n",
      " [ 0.0496  0.2028  0.0286 -0.3235]\n",
      " [ 0.045  -0.0008 -0.0308  0.0352]] [False False  True] [{}, {}, {'actual_next_step': array([ 0.1099,  1.6147, -0.2193, -2.535 ], dtype=float32), 'terminal_observation': array([ 0.1099,  1.6147, -0.2193, -2.535 ], dtype=float32)}]\n",
      "36\n",
      "36 [[-0.0127  0.3583 -0.0086 -0.5636]\n",
      " [ 0.0537  0.3975  0.0221 -0.607 ]\n",
      " [ 0.0449  0.1947 -0.0301 -0.2671]] [[-0.0127  0.3583 -0.0086 -0.5636]\n",
      " [ 0.0537  0.3975  0.0221 -0.607 ]\n",
      " [ 0.0449  0.1947 -0.0301 -0.2671]] [False False False] [{}, {}, {}]\n",
      "37\n",
      "37 [[-0.0055  0.5535 -0.0199 -0.8589]\n",
      " [ 0.0616  0.5924  0.01   -0.8927]\n",
      " [ 0.0488  0.3903 -0.0355 -0.5691]] [[-0.0055  0.5535 -0.0199 -0.8589]\n",
      " [ 0.0616  0.5924  0.01   -0.8927]\n",
      " [ 0.0488  0.3903 -0.0355 -0.5691]] [False False False] [{}, {}, {}]\n",
      "38\n",
      "38 [[ 0.0056  0.7489 -0.037  -1.1578]\n",
      " [ 0.0734  0.7873 -0.0079 -1.1822]\n",
      " [ 0.0566  0.5859 -0.0469 -0.8727]] [[ 0.0056  0.7489 -0.037  -1.1578]\n",
      " [ 0.0734  0.7873 -0.0079 -1.1822]\n",
      " [ 0.0566  0.5859 -0.0469 -0.8727]] [False False False] [{}, {}, {}]\n",
      "39\n",
      "39 [[ 0.0206  0.9445 -0.0602 -1.4619]\n",
      " [ 0.0892  0.9826 -0.0315 -1.4773]\n",
      " [ 0.0684  0.7816 -0.0643 -1.1798]] [[ 0.0206  0.9445 -0.0602 -1.4619]\n",
      " [ 0.0892  0.9826 -0.0315 -1.4773]\n",
      " [ 0.0684  0.7816 -0.0643 -1.1798]] [False False False] [{}, {}, {}]\n",
      "40\n",
      "40 [[ 0.0394  1.1403 -0.0894 -1.7727]\n",
      " [ 0.1088  1.1781 -0.0611 -1.7797]\n",
      " [ 0.084   0.9775 -0.0879 -1.4919]] [[ 0.0394  1.1403 -0.0894 -1.7727]\n",
      " [ 0.1088  1.1781 -0.0611 -1.7797]\n",
      " [ 0.084   0.9775 -0.0879 -1.4919]] [False False False] [{}, {}, {}]\n",
      "41\n",
      "41 [[ 0.0623  1.3363 -0.1249 -2.0918]\n",
      " [ 0.1324  1.3738 -0.0967 -2.0907]\n",
      " [ 0.1035  1.1736 -0.1178 -1.8107]] [[ 0.0623  1.3363 -0.1249 -2.0918]\n",
      " [ 0.1324  1.3738 -0.0967 -2.0907]\n",
      " [ 0.1035  1.1736 -0.1178 -1.8107]] [False False False] [{}, {}, {}]\n",
      "42\n",
      "42 [[ 0.089   1.5324 -0.1667 -2.4203]\n",
      " [ 0.1599  1.5698 -0.1385 -2.4117]\n",
      " [ 0.127   1.3698 -0.154  -2.1375]] [[ 0.089   1.5324 -0.1667 -2.4203]\n",
      " [ 0.1599  1.5698 -0.1385 -2.4117]\n",
      " [ 0.127   1.3698 -0.154  -2.1375]] [False False False] [{}, {}, {}]\n",
      "43\n",
      "43 [[-0.0008 -0.0139  0.0198 -0.0135]\n",
      " [ 0.1913  1.7658 -0.1867 -2.7435]\n",
      " [ 0.1544  1.5661 -0.1967 -2.4736]] [[-0.0008 -0.0139  0.0198 -0.0135]\n",
      " [ 0.1913  1.7658 -0.1867 -2.7435]\n",
      " [ 0.1544  1.5661 -0.1967 -2.4736]] [ True False False] [{'actual_next_step': array([ 0.1196,  1.7286, -0.2151, -2.7592], dtype=float32), 'terminal_observation': array([ 0.1196,  1.7286, -0.2151, -2.7592], dtype=float32)}, {}, {}]\n",
      "44\n",
      "44 [[-0.0011  0.181   0.0196 -0.2998]\n",
      " [-0.0392  0.0498 -0.0436  0.0431]\n",
      " [ 0.0185  0.0021  0.0352 -0.0474]] [[-0.0011  0.181   0.0196 -0.2998]\n",
      " [-0.0392  0.0498 -0.0436  0.0431]\n",
      " [ 0.0185  0.0021  0.0352 -0.0474]] [False  True  True] [{}, {'actual_next_step': array([ 0.2266,  1.9617, -0.2416, -3.0868], dtype=float32), 'terminal_observation': array([ 0.2266,  1.9617, -0.2416, -3.0868], dtype=float32)}, {'actual_next_step': array([ 0.1857,  1.7622, -0.2462, -2.8196], dtype=float32), 'terminal_observation': array([ 0.1857,  1.7622, -0.2462, -2.8196], dtype=float32)}]\n",
      "45\n",
      "45 [[ 0.0025  0.3758  0.0136 -0.5863]\n",
      " [-0.0383  0.2455 -0.0427 -0.263 ]\n",
      " [ 0.0186  0.1967  0.0343 -0.3287]] [[ 0.0025  0.3758  0.0136 -0.5863]\n",
      " [-0.0383  0.2455 -0.0427 -0.263 ]\n",
      " [ 0.0186  0.1967  0.0343 -0.3287]] [False False False] [{}, {}, {}]\n",
      "46\n",
      "46 [[ 0.01    0.5707  0.0018 -0.8747]\n",
      " [-0.0333  0.4412 -0.048  -0.5688]\n",
      " [ 0.0225  0.3914  0.0277 -0.6104]] [[ 0.01    0.5707  0.0018 -0.8747]\n",
      " [-0.0333  0.4412 -0.048  -0.5688]\n",
      " [ 0.0225  0.3914  0.0277 -0.6104]] [False False False] [{}, {}, {}]\n",
      "47\n",
      "47 [[ 0.0214  0.7658 -0.0157 -1.1668]\n",
      " [-0.0245  0.637  -0.0594 -0.8762]\n",
      " [ 0.0303  0.5861  0.0155 -0.8942]] [[ 0.0214  0.7658 -0.0157 -1.1668]\n",
      " [-0.0245  0.637  -0.0594 -0.8762]\n",
      " [ 0.0303  0.5861  0.0155 -0.8942]] [False False False] [{}, {}, {}]\n",
      "48\n",
      "48 [[ 0.0367  0.9612 -0.039  -1.4643]\n",
      " [-0.0118  0.8329 -0.0769 -1.187 ]\n",
      " [ 0.042   0.781  -0.0024 -1.182 ]] [[ 0.0367  0.9612 -0.039  -1.4643]\n",
      " [-0.0118  0.8329 -0.0769 -1.187 ]\n",
      " [ 0.042   0.781  -0.0024 -1.182 ]] [False False False] [{}, {}, {}]\n",
      "49\n",
      "49 [[ 0.056   1.1567 -0.0683 -1.7689]\n",
      " [ 0.0049  1.0289 -0.1006 -1.5027]\n",
      " [ 0.0577  0.9761 -0.026  -1.4754]] [[ 0.056   1.1567 -0.0683 -1.7689]\n",
      " [ 0.0049  1.0289 -0.1006 -1.5027]\n",
      " [ 0.0577  0.9761 -0.026  -1.4754]] [False False False] [{}, {}, {}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}, {}, {}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "nn = 3\n",
    "\n",
    "env_vect = gym.vector.SyncVectorEnv([lambda: gym.make(\"CartPole-v1\").env for _ in range(nn)])\n",
    "\n",
    "print(\"TYPE 1\", type(env_vect.observations))\n",
    "current_state = env_vect.reset()\n",
    "\n",
    "print(\"current_state\", current_state)\n",
    "#print(\"self.env.state\", env_vect.state)\n",
    "print(\"self.env.state\", env_vect.observations)\n",
    "\n",
    "a = np.array([[-10.0,  0.4157, -0.0285, -0.6019],\n",
    "        [ 10.0,  0.3979, -0.0239, -0.6019],\n",
    "        [ 10.0,  0.8187, -0.072 , -1.2578]])\n",
    "\n",
    "env_vect.observations = a\n",
    "\n",
    "print(\"TYPE 2\", type(env_vect.observations))\n",
    "print(\"env_vect.observations\", env_vect.observations)\n",
    "\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    next_state, reward , done, info = env_vect.step([1 for i in range(nn)])\n",
    "    \n",
    "    print(i, env_vect.observations, next_state, done, info) \n",
    "\n",
    "    #print(next_state, reward , done, info)\n",
    "        \n",
    "    #print(\"a\", env_vect.observations, \"b\", current_state, \"c\", np.array(current_state) / np.array(env_vect.observations))\n",
    "    #print(next_state / current_state, done, current_state, next_state, env_vect.observations)\n",
    "    #print(current_state / next_state, done, current_state, next_state, env_vect.observations, info)    \n",
    "    #print(np.array(current_state) / np.array(next_state))\n",
    "    #print(\"env_vect.observations\", env_vect.observations)\n",
    "    current_state = deepcopy(next_state)\n",
    "    #current_state = deepcopy(env_vect.observations)\n",
    "    #print(\"current_state\", current_state)\n",
    "    \n",
    "info    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1084ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'numpy.ndarray'> [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "1 <class 'numpy.ndarray'> [[ 0.0381 -0.013   0.0499  0.0083]\n",
      " [-0.0254  0.0201  0.0439 -0.02  ]\n",
      " [-0.0232  0.0463 -0.0127 -0.0123]]\n",
      "1a [ 0.0381 -0.013   0.0499  0.0083] <class 'numpy.ndarray'>\n",
      "1b [0.1 0.2 0.3 0.4] <class 'numpy.ndarray'>\n",
      "1c [0.1 0.2 0.3 0.4] <class 'numpy.ndarray'>\n",
      "1a [-0.0254  0.0201  0.0439 -0.02  ] <class 'numpy.ndarray'>\n",
      "1b [0.1 0.2 0.3 0.4] <class 'numpy.ndarray'>\n",
      "1c [0.1 0.2 0.3 0.4] <class 'numpy.ndarray'>\n",
      "1a [-0.0232  0.0463 -0.0127 -0.0123] <class 'numpy.ndarray'>\n",
      "1b [0.1 0.2 0.3 0.4] <class 'numpy.ndarray'>\n",
      "1c [0.1 0.2 0.3 0.4] <class 'numpy.ndarray'>\n",
      "2 [[ 0.0381 -0.013   0.0499  0.0083]\n",
      " [-0.0254  0.0201  0.0439 -0.02  ]\n",
      " [-0.0232  0.0463 -0.0127 -0.0123]]\n",
      "2a [0.1 0.2 0.3 0.4]\n",
      "2a [0.1 0.2 0.3 0.4]\n",
      "2a [0.1 0.2 0.3 0.4]\n",
      "(array([[ 0.0415,  0.3758,  0.0447, -0.5447],\n",
      "       [-0.0207,  0.409 ,  0.0375, -0.5772],\n",
      "       [-0.0174,  0.437 , -0.0191, -0.6057]], dtype=float32), array([1., 1., 1.]), array([False, False, False]), [{}, {}, {}])\n",
      "3 ((0.11, 0.12, 0.13, 0.14), (0.11, 0.12, 0.13, 0.14), (0.11, 0.12, 0.13, 0.14))\n",
      "3a [0.1 0.2 0.3 0.4]\n",
      "3a [0.1 0.2 0.3 0.4]\n",
      "3a [0.1 0.2 0.3 0.4]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'out' must be an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/d_xxpqw56t95qlbw1w07l3340000gn/T/ipykernel_4424/1988019023.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#env_vect.step([1 for i in range(nn)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0menv_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/vector/vector_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/vector/sync_vector_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         self.observations = concatenate(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_observation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         )\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    886\u001b[0m                             '1 positional argument')\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/vector/utils/numpy_utils.py\u001b[0m in \u001b[0;36m_concatenate_base\u001b[0;34m(space, items, out)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiBinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_concatenate_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'out' must be an array"
     ]
    }
   ],
   "source": [
    "nn = 3\n",
    "\n",
    "env_vect = gym.vector.SyncVectorEnv([lambda: gym.make(\"CartPole-v1\").env for _ in range(nn)])\n",
    "print(0, type(env_vect.observations), env_vect.observations)\n",
    "env_vect.reset()\n",
    "\n",
    "print(1, type(env_vect.observations), env_vect.observations)\n",
    "\n",
    "# SET EACH ENV INDIVIDUALY\n",
    "for env in env_vect.envs:\n",
    "    print(\"1a\", env.state, type(env.state))\n",
    "    env.state = np.array([.1,.2,.3,.4])\n",
    "    #env.state = (.1,.2,.3,.4)\n",
    "    print(\"1b\", env.state, type(env.state))\n",
    "    env.step(1)\n",
    "    print(\"1c\", env.state, type(env.state))\n",
    "\n",
    "print(2, env_vect.observations)\n",
    "\n",
    "for env in env_vect.envs:\n",
    "    print(\"2a\", env.state)\n",
    "    \n",
    "print(env_vect.step([1 for i in range(nn)]))    \n",
    "    \n",
    "#env_vect.observations = np.array([[11,12,13,14], [11,12,13,14],[11,12,13,14]]).astype(float)\n",
    "env_vect.observations = ((.11,.12,.13,.14), (.11,.12,.13,.14),(.11,.12,.13,.14))\n",
    "\n",
    "print(3, env_vect.observations)\n",
    "\n",
    "for env in env_vect.envs:\n",
    "    print(\"3a\", env.state)\n",
    "    \n",
    "#env_vect.step([1 for i in range(nn)])\n",
    "env_vect.step([1,1,1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[-10.0,  0.4157, -0.0285, -0.6019],\n",
    "        [ 10.0,  0.3979, -0.0239, -0.6019],\n",
    "        [ 10.0,  0.8187, -0.072 , -1.2578]])\n",
    "\n",
    "env_vect.observations = a\n",
    "\n",
    "env_vect.observations, env_vect.envs, dir(env_vect), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 1\n",
    "\n",
    "#env_vect = gym.vector.SyncVectorEnv([lambda: gym.make(\"CartPole-v1\").env for _ in range(nn)])\n",
    "env_vect = gym.vector.make('CartPole-v1', num_envs=nn)\n",
    "\n",
    "current_state = env_vect.reset()\n",
    "\n",
    "print(\"current_state\", current_state)\n",
    "#print(\"self.env.state\", env_vect.state)\n",
    "print(\"self.env.state\", env_vect.observations)\n",
    "\n",
    "for i in range(50):\n",
    "    next_state, reward , done, info = env_vect.step([1 for i in range(nn)])\n",
    "    print(current_state / next_state, done, current_state, next_state, env_vect.observations)\n",
    "    current_state = deepcopy(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"dir(env_vect)\", env_vect.observations, env_vect.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in env_vect:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd278ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(env_vect), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e648d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install procgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "200162da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebtac/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/sebtac/miniforge3/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/sebtac/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/sebtac/miniforge3/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/sebtac/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/sebtac/miniforge3/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ConcatEnv' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/d_xxpqw56t95qlbw1w07l3340000gn/T/ipykernel_4424/784251780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.4157\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0285\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.6019\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0menvi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0menvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ConcatEnv' object is not iterable"
     ]
    }
   ],
   "source": [
    "import gym3\n",
    "nn = 3\n",
    "#env = gym3.vectorize_gym(num=3, render_mode=\"human\", env_kwargs={\"id\": \"CartPole-v0\"}) # it will render the game each time you run code below.\n",
    "\n",
    "env = gym3.vectorize_gym(num=3, render_mode=None, env_kwargs={\"id\": \"CartPole-v0\"})\n",
    "\n",
    "a = np.array([[-1.5,  0.4157, -0.0285, -0.6019],\n",
    "        [ 1.5,  0.3979, -0.0239, -0.6019],\n",
    "        [ 1.5,  0.8187, -0.072 , -1.2578]])\n",
    "\n",
    "a = np.array([-1.5,  0.4157, -0.0285, -0.6019])\n",
    "\n",
    "#for envi in env:\n",
    "#    envi.set_attr(\"state\", a)\n",
    "\n",
    "current_state = env.observe()[1]\n",
    "print(\"current_state\", current_state)\n",
    "step = 0\n",
    "while step < 50:\n",
    "    #env.act(gym3.types_np.sample(env.ac_space, bshape=(env.num,)))\n",
    "    env.act([1 for i in range(nn)])\n",
    "    reward, next_state, first = env.observe()\n",
    "    print(f\"step {step} reward {reward} first {first}\", current_state, next_state, current_state/next_state)\n",
    "    \n",
    "    current_state = env.observe()[1]\n",
    "    \n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0b28196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'append',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'extend',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'pop',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'sort']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(env.envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import __file__ as gname\n",
    "gname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7aae4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_state [[ 0.0371  0.0055  0.0147  0.0058]\n",
      " [ 0.008  -0.033  -0.0367 -0.0252]\n",
      " [-0.0295  0.0158 -0.0021  0.0491]]\n",
      "self.env.state [[ 0.0371  0.0055  0.0147  0.0058]\n",
      " [ 0.008  -0.033  -0.0367 -0.0252]\n",
      " [-0.0295  0.0158 -0.0021  0.0491]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AsyncVectorEnv' object has no attribute 'envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/d_xxpqw56t95qlbw1w07l3340000gn/T/ipykernel_4424/3741667409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# SET EACH ENV INDIVIDUALY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(\"1a\", env.state, type(env.state))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AsyncVectorEnv' object has no attribute 'envs'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "nn = 3\n",
    "\n",
    "#env_vect = gym.vector.SyncVectorEnv([lambda: gym.make(\"CartPole-v1\").env for _ in range(nn)])\n",
    "\n",
    "env_vect = gym.vector.make(\"CartPole-v1\", num_envs=3, asynchronous=True)\n",
    "#vec_env.set_attr(\"state\", np.array([0, 1, 2, 3]))\n",
    "\n",
    "current_state = env_vect.reset()\n",
    "\n",
    "print(\"current_state\", current_state)\n",
    "print(\"self.env.state\", env_vect.observations)\n",
    "\n",
    "a = np.array([[-1.5,  0.4157, -0.0285, -0.6019],\n",
    "        [ 1.5,  0.3979, -0.0239, -0.6019],\n",
    "        [ 1.5,  0.8187, -0.072 , -1.2578]])\n",
    "\n",
    "# SET EACH ENV INDIVIDUALY\n",
    "for env in env_vect.envs:\n",
    "    env.set_attr(\"state\", a)\n",
    "    #print(\"1a\", env.state, type(env.state))\n",
    "    #env.state = np.array([.1,.2,.3,.4])\n",
    "    #env.state = (.1,.2,.3,.4)\n",
    "    #print(\"1b\", env.state, type(env.state))\n",
    "    #env.step(1)\n",
    "    #print(\"1c\", env.state, type(env.state))\n",
    "    \n",
    "#env_vect.set_attr(\"state\", a)\n",
    "#env_vect.set_attr(\"observations\", a)\n",
    "#env_vect.observations = a\n",
    "\n",
    "print(\"env_vect.observations\", env_vect.observations)\n",
    "\n",
    "for i in range(50):\n",
    "    next_state, reward , done, info = env_vect.step([1 for i in range(nn)])\n",
    "    current_state = deepcopy(next_state)    \n",
    "    print(i, env_vect.observations, next_state, done, info) \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a54a4ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " 3,\n",
       " ['__annotations__',\n",
       "  '__class__',\n",
       "  '__class_getitem__',\n",
       "  '__del__',\n",
       "  '__delattr__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__enter__',\n",
       "  '__eq__',\n",
       "  '__exit__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__le__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__orig_bases__',\n",
       "  '__parameters__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__setattr__',\n",
       "  '__sizeof__',\n",
       "  '__slots__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__',\n",
       "  '_assert_is_running',\n",
       "  '_check_spaces',\n",
       "  '_is_protocol',\n",
       "  '_np_random',\n",
       "  '_poll',\n",
       "  '_raise_if_errors',\n",
       "  '_state',\n",
       "  'action_space',\n",
       "  'call',\n",
       "  'call_async',\n",
       "  'call_wait',\n",
       "  'close',\n",
       "  'close_extras',\n",
       "  'closed',\n",
       "  'copy',\n",
       "  'env_fns',\n",
       "  'error_queue',\n",
       "  'get_attr',\n",
       "  'is_vector_env',\n",
       "  'metadata',\n",
       "  'np_random',\n",
       "  'num_envs',\n",
       "  'observation_space',\n",
       "  'observations',\n",
       "  'parent_pipes',\n",
       "  'processes',\n",
       "  'render',\n",
       "  'reset',\n",
       "  'reset_async',\n",
       "  'reset_wait',\n",
       "  'reward_range',\n",
       "  'seed',\n",
       "  'set_attr',\n",
       "  'shared_memory',\n",
       "  'single_action_space',\n",
       "  'single_observation_space',\n",
       "  'spec',\n",
       "  'step',\n",
       "  'step_async',\n",
       "  'step_wait',\n",
       "  'unwrapped',\n",
       "  'viewer'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_vect.spec, env_vect.num_envs, dir(env_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d6acd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.env.state [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Received the following error from Worker-0: AssertionError: Cannot call env.step() before calling reset()\u001b[0m\n",
      "\u001b[31mERROR: Shutting down Worker-0.\u001b[0m\n",
      "\u001b[31mERROR: Received the following error from Worker-2: AssertionError: Cannot call env.step() before calling reset()\u001b[0m\n",
      "\u001b[31mERROR: Shutting down Worker-2.\u001b[0m\n",
      "\u001b[31mERROR: Received the following error from Worker-1: AssertionError: Cannot call env.step() before calling reset()\u001b[0m\n",
      "\u001b[31mERROR: Shutting down Worker-1.\u001b[0m\n",
      "\u001b[31mERROR: Raising the last exception back to the main process.\u001b[0m\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Cannot call env.step() before calling reset()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/d_xxpqw56t95qlbw1w07l3340000gn/T/ipykernel_4424/4067227287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/vector/vector_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/vector/async_vector_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_pipes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mobservations_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/vector/async_vector_env.py\u001b[0m in \u001b[0;36m_raise_if_errors\u001b[0;34m(self, successes)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Raising the last exception back to the main process.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexctype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cannot call env.step() before calling reset()"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "nn = 3\n",
    "\n",
    "a = np.array([[-1.5,  0.4157, -0.0285, -0.6019],\n",
    "        [ 1.5,  0.3979, -0.0239, -0.6019],\n",
    "        [ 1.5,  0.8187, -0.072 , -1.2578]])\n",
    "\n",
    "#env_vect = gym.vector.SyncVectorEnv([lambda: gym.make(\"CartPole-v1\").env for _ in range(nn)])\n",
    "\n",
    "env_vect = gym.vector.make(\"CartPole-v1\", num_envs=3, asynchronous=True)\n",
    "env_vect.set_attr(\"state\", a)\n",
    "\n",
    "#current_state = env_vect.reset()\n",
    "\n",
    "#print(\"current_state\", current_state)\n",
    "print(\"self.env.state\", env_vect.observations)\n",
    "\n",
    "#env_vect.observations = a\n",
    "\n",
    "#print(\"env_vect.observations\", env_vect.observations)\n",
    "\n",
    "for i in range(50):\n",
    "    next_state, reward , done, info = env_vect.step([1 for i in range(nn)])\n",
    "    current_state = deepcopy(next_state)    \n",
    "    print(i, env_vect.observations, next_state, done, info) \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce649074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
